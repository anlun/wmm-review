\section{Criteria for Memory Models}
\label{sec:background}

In this section we will have a closer look on criteria for 
programming language memory models, 
namely an optimality of the compilation scheme, 
soundness of common program transformations, 
and provided reasoning guarantees.  
% But first we need to specify 

\subsection{Abstractions and Interfaces}

The memory model defines the semantics of the shared memory 
in the presence of concurrently executing threads. 
More concretly, the memory model 
defines which values reads of shared variables 
can observe at each point of execution. 
Therefore the main abstraction of the memory model 
is the shared memory itself. 

The shared memory consists of a individual shared variables, 
each having unique address~\footnote{
Throughout the rest of paper, we use terms 
memory address and memory location interchangeably}.
Threads can access these variables by performing 
loads or stores~\footnote{We also use the terms 
load/stores and reads/writes interchangeably}.

% \begin{minipage}{0.45\linewidth}
% \begin{equation*}
%   \readInst{}{r}{x}     
% \tag{Load}\label{ex:load}
% \end{equation*}
% \end{minipage}\hfill%
% \begin{minipage}{0.45\linewidth}
% \begin{equation*}
%   \writeInst{}{x}{1}    
% \tag{Store}\label{ex:store}
% \end{equation*}
% \end{minipage}

Most programming language memory models distinguish 
\emph{non-atomic} (sometimes also called \emph{plain})
and \emph{atomic} variables. 
The former generally should not be accessed 
concurrently from parallel threads. 
Depending on the particular programming language, 
concurrent accesses to non-atomic variables 
can be either prevented by the type-system 
(\eg \Haskell~\cite{Marlow-al:Haskell10, Vollmer-al:PPoPP17}, \Rust~\cite{RustBook:19}), 
have undefined behavior (\eg \CPP~\cite{Boehm-Adve:PLDI08, Batty-al:POPL11}), 
or have defined but very weak semantics with almost 
no guarantees on the order of accesses to non-atomics
that concurrent threads can observe (\eg \Java~\cite{Manson-al:POPL05}).

In addition to this some memory models distinguish 
several kinds of accesses to atomic variables.
In these models the accesses to shared memory are annotated by the 
so-called \emph{access modes}.
For example, the \CPP model (and a later revision of 
\Java~\cite{Bender-Palsberg:OOPSLA19}), distinguish 
three modes: \emph{relaxed} (\emph{opaque} in \Java terminology), 
\emph{acquire/release}, and \emph{sequentially consistent}.
They are denoted as $\rlx$, $\opq$, $\acqrel$, and $\sco$ correspondingly.
Non-atomic accesses are often considered to be the fourth access mode $\na$, 
but note that mixing of non-atomic and atomic accesses to the same variable 
entails undefined behavior in \CPP.
The access modes are ordered by the guarantees they provide
in exchange of optimization opportunities, as the following 
diagram shows.

$$ \na \sqsubseteq \rlx \sqsubseteq \acqrel \sqsubseteq \sco $$

On the one end of the spectrum are sequentially consistent accesses. 
They guarantee to restore the \SC semantics, if used properly 
(see \cref{sec:background-drf} for details).
Non-atomic accesses, as we have already discussed, give 
from little to no guarantee. 
Relaxed accesses also have very weak guarantees, 
usually they provide the so-called \emph{coherence} property
(see \cref{sec:background-coh} for details).
Finally, in the middle there are the acquire/release accesses. 
They are designed to support the message passing idiom~\cite{Lahav-al:POPL16}.
The thread sends the message by performing a release write, 
the thread expecting a message can perform an acquire read. 
If the acquire read observes the release write, the two 
threads ``synchronize''. 



% Taking as an examples three memory models: 
% sequential consistency, Java memory model and C/C++ memory model
% we will see that each of them fails at least in one aspect.

%% EVGENII: also cite corresponding java and c/c++ specs?

% \subsection{Memory models under consideration}

% We chose to demonstrate the requirements to models on the example of SC, CMM and JMM for reason.
% Again, SC was picked as a ``baseline''.
% The other two models, Java and C/C++, was chosen because 
% (1) they were the first formally specified memory models of industrial programming languages, and
% (2) they are significantly differ in their design and goals.

% \subsubsection{C/C++ Memory Model}

% Memory model of the C/C++ follows the design principles of the language itself.

% C and C++ languages positions themselves as low-level programming languages
% that provide zero-cost abstractions that generally do not put any performance penalties. 
% In other words the abstraction that these languages provide 
% should be compiled into efficient assembly code,
% and leave the room for the aggressive optimizations.

% The efficiency of the compiled code however comes with a certain cost.
% The programmer should strictly obey the rules and conventions
% established by the language's specification, 
% otherwise program is said to have \emph{undefined behavior} (UB for short)

% With respect to the memory model, the above means that C/C++ compiler should
% compile accesses to shared variables as plain memory accesses of the CPU,
% and provide to the programmer low-level syncronization primitives
% that can be mapped directly to CPU instructions.
% Besides that, the compiler should be able to perform 
% as many optimizations as possible to the code containing shared accesses.
% As we will see, not all of the optimizations that are sound 
% for a single thread program reamins sound for concurrent programs that contain data races. 
% %% TODO: give some example? reordering of reads to the same location, as Anton suggested? 

% For these reasons C/C++ distinguish two kind of memory accesses.
% Non-atomic accesses are can be used to perform read or write to memory that 
% is owned exclusively by one thread at the time of the access.
% Data-races on non-atomic accesses lead to undefined behavior for the whole program
% (so called \emph{catch-fire} semantics).
% In contrast, atomic accesses can be used to access memory that can be shared between different threads. 
% The results of data-races on these accesses are specified by the language and do not lead to undefined behavior.
% The consequence is that some of the compiler optimization are not applicable to atomic accesses.
% In addition to two kinds of memory accesses C/C++ also provide fences ---
% low-level synchronization primitives similar to CPUs fence instructions.

% \subsubsection{Java Memory Model}

% The Java language makes different design choices and has different tradeoffs comparing to C/C++.
% Unlike the later languages, Java provides safety and security guarantees
% which are enforced both at compile time and at runtime.  
% Thus Java language specification cannot tolerate undefined behaviors.
% Despite that the Java compilers still make a lot of effort 
% to provide good performance of the compiled code.

% Consequently, Java memory model should follow this design 
% and do not break any of the language's guarantees
% while still admit fairly efficient compilation scheme
% and allow as many optimizations as possible.

\subsection{Optimal Compilation Scheme}

\subsubsection{Full Memory Fences}

\begin{itemize}
  \item Mention x86-TSO again.
  \item Present SB example again. 
  \item How to restore SC?
  \item Introduce \texttt{mfence}. SB+MFENCE example.
  \item Mention \texttt{sync} (POWER) and \texttt{dmb} (ARM).
\end{itemize}

\subsubsection{Lightweight Memory Fences}

\begin{itemize}
  \item Mention POWER and ARM (weaker than x86).
  \item Present MP example. 
  \item Introduce \texttt{lwsync} (POWER), \texttt{dmb ld} and \texttt{dmb st} (ARM).
\end{itemize}

\subsubsection{Syntactic Dependencies}

\begin{itemize}
  \item Mention POWER and ARM again. Mention event weaker archs (e.g. DEC-Alpha) (?).
  \item Present LB example. 
  \item Introduce syntactic dependencies and \texttt{isync} (POWER).
\end{itemize}

% Having a short description of the memory models 
% together with their design goals we are ready to proceed 
% and consider whether these models satisfy the desired requirements.
% We are going to start with compilation schemes.

% As we have seen, the memory models of modern CPUs 
% (those based on x86, ARMv8 and POWER architecture)
% are weak and allow non sequentially consistent behaviors.
% This is a result of various optimization implemented in hardware,
% including instruction pipelines, speculative out of order executions, 
% hierarchy of caches with complex coherence protocols, and others.

% If the memory model wants to provide stronger guarantees 
% than the CPU does (as for example sequentially consistent model)
% it should somehow prevent the out of order executions.
% In general, there are two ways to achieve that. 

% First, as we have already discussed, special fence instructions,
% provided by the CPU (such as \texttt{mfence} on x86) can be used.
% These instructions usually flush caches, prevent speculative executions
% and perform any other actions required by the hardware architecture
% to forbid various weak behaviours.

% Second, all modern hardware architectures do not reorder instructions 
% if there are \emph{syntactic dependencies} between them. 
% For example, the load instruction cannot be moved below 
% the store instruction if there is conditional jump instruction between them.
% The compiler can utilize this and prevent 
% the reordering of the load instruction below subsequent stores
% by emiting a usuless conditional jump instruction that would jump 
% to the same label no matter what is the result of condition evaluation.
% Dependencies of such kind can be computed following the 
% syntax of the program (hence the name) as opposed 
% to \emph{semantic dependencies} 
% (we will see the difference between the two later).

% Now that we have CPU fences and syntactic dependencies in our service
% let's have a look at how they are used in real compilation schemes.

% \subsubsection{Compiling SC}

% Sequential consistency model is very expensive to implement in hardware. 
% For this reasons all modern hardware architectures (including rather strong x86) do not provide it. 
% We have seen this on the examples of Dekker lock and SB programs in \ref{introduction}.

% In order to restore the sequential consistency on x86 one has to 
% insert \texttt{mfence} instruction after each write.
% On ARMv7 and POWER one need to insert full memory fence
% (\texttt{dmb} and \texttt{sync} on ARMv7/POWER correspondingly)
% before each write,
% emit same full fence before each read, and also
% add a syntactic dependency with special instruction fence 
% (\texttt{isb} and \texttt{isync} on ARMv7/POWER correspondingly)
% after the read.
% Newer ARMv8 chips allows simpler solution, 
% one just need to compile accesses to special 
% load/store instruction to restore SC semantics
% (\texttt{ldar} and \texttt{stlr}).
% The table summarizes the resulting compilation mappings.

% %% TODO: Here will be the table with compilation mappings

% The natural question to ask is how much of the performance penalties
% these compilation mappings induce compared to compiling all accesses as plain ones
% without any fences or artificial syntactic dependencies.

% %% TODO: cite some paper, present table with measurements etc. 

% Thus one can see that enforcing sequential consistency on modern hardware is costly.

% \subsubsection{Compiling C/C++}

% \subsubsection{Compiling Java}

%% TODO: compiler optimizations --> program transformations (?)

\subsection{Soundness of Program Transformations}
\label{sec:bgrnd-opt-sound}

\subsubsection{Local Transformations}

\paragraph{Trace preserving transformations (TR)}

\paragraph{Reordering of independent instructions (RI)}

\paragraph{Redundunt Load/Store Elimination (RE)}

\paragraph{Irrelevant Load Elimination (ILE)}

\paragraph{Speculative Load Introduction (SLI)}

\paragraph{Strengthening (S)}

\paragraph{Roach Motel Reordering (RM)}

\subsubsection{Global Transformations}

\paragraph{Register Promotion (RP)}

\paragraph{Value Range based Transformations (VR)}

\paragraph{Thread Inlining (TI)}

\subsection{Reasoning Guarantees}

\subsubsection{DRF Theorems}
\label{sec:background-drf}

\begin{itemize}
  \item Motivation: no weak behaviors for well-sync programs.
  \item Example
  \begin{itemize}
    \item Lock + increment?
    \item MP with atomic flag?
  \end{itemize}
  \item Definition of the race
  \item DRF theorem
  \begin{itemize}
    \item explain difference between internal/external DRF
  \end{itemize}
  \item Local DRF (?)
  \begin{itemize}
    \item Example
  \end{itemize}
\end{itemize}

\subsubsection{Out-of-Thin-Air Values}
\label{sec:bgrnd-oota}

\begin{itemize}
  \item Introduce three examples: LB, LB+dep, LB+fakedep.
  \item Explain the problem: some memory models cannot 
    correctly distinguish theese three programs.
  \item It leads to out-of-thin-air values.
\end{itemize}

% \begin{equation*}
% \inarrII{
%   \readInst{}{r_1}{x}      \\
%   \writeInst{}{y}{r_1}     \\
% }{
%   \readInst{}{r_2}{x}      \\
%   \writeInst{}{x}{r_2}     \\
% }
% \tag{LB+data}\label{ex:lb+data}
% \end{equation*}


\subsubsection{$\lPO\cup\lRF$ Acyclicity}

\begin{itemize}
  \item Motivation: forbid OOTA.
  \item Proposed solution: forbid causality cycles. 
  \item Explain execution graphs and $\lPO\cup\lRF$ cycles.
  \item LB examples againg, with their execution graphs.
  \item Drawback of this approach: some optimizations are unsound 
    (refer to analysis section).
\end{itemize}

\subsubsection{Syntactic Dependency Tracking}

\begin{itemize}
  \item Motivation: forbid OOTA, but enable more opts.
  \item Proposed solution: forbid causality cycles with syntactic dependencies. 
  \item Explain $\lPPO\cup\lRF$ cycles.
  \item LB examples againg, with their execution graphs and syntax.deps.
  \item Drawback of this approach: some optimizations are still unsound 
    (refer to analysis section).
\end{itemize}

\subsubsection{Semantic Dependency Tracking}

\begin{itemize}
  \item Motivation: forbid OOTA (precisely).
  \item Proposed solution: forbid causality cycles with semantic dependencies. 
  \item The exact notion of semantic dependency is model-specific.
        There is no conventional common definition.
  \item Drawback of this approach: complexity, no common ground.
\end{itemize}

\subsubsection{Undefined Behavior}
\label{sec:bgrnd-ub}

\begin{itemize}
  \item Motivation: enable more opts for non-atomics.
        (At the cost of model's predictability and programmability).
  \item Catch-fire semantics (race on non-atomics implies UB).
  \item Explain how it affects opts.
\end{itemize}

\subsubsection{Coherence (?)}
\label{sec:background-coh}

\subsection{Supported Features and Interfaces}

