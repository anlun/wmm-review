\section{Background on Programming Language Memory Models}

In this section we will have a closer look into the requirements 
that a programming language memory model should satisfy, 
namely the existence of sound and efficient compilation scheme, 
soundness of common optimizations, and guarantees for formal and informal reasoning.  
Taking as an examples three memory models: 
sequential consistency, Java memory model and C/C++ memory model
we will see that each of them fails at least in one aspect.
%% EVGENII: also cite corresponding java and c/c++ specs?

\subsection{Memory models under consideration}

We chose to demonstrate the requirements to models on the example of SC, CMM and JMM for reason.
Again, SC was picked as a ``baseline''.
The other two models, Java and C/C++, was chosen because 
(1) they were the first formally specified memory models of industrial programming languages, and
(2) they are significantly differ in their design and goals.

\subsubsection{C/C++ Memory Model}

Memory model of the C/C++ follows the design principles of the language itself.

C and C++ languages positions themselves as low-level programming languages
that provide zero-cost abstractions that generally do not put any performance penalties. 
In other words the abstraction that these languages provide 
should be compiled into efficient assembly code,
and leave the room for the aggressive optimizations.

The efficiency of the compiled code however comes with a certain cost.
The programmer should strictly obey the rules and conventions
established by the language's specification, 
otherwise program is said to have \emph{undefined behavior} (UB for short)

With respect to the memory model, the above means that C/C++ compiler should
compile accesses to shared variables as plain memory accesses of the CPU,
and provide to the programmer low-level syncronization primitives
that can be mapped directly to CPU instructions.
Besides that, the compiler should be able to perform 
as many optimizations as possible to the code containing shared accesses.
As we will see, not all of the optimizations that are sound 
for a single thread program reamins sound for concurrent programs that contain data races. 
%% TODO: give some example? reordering of reads to the same location, as Anton suggested? 

For these reasons C/C++ distinguish two kind of memory accesses.
Non-atomic accesses are can be used to perform read or write to memory that 
is owned exclusively by one thread at the time of the access.
Data-races on non-atomic accesses lead to undefined behavior for the whole program
(so called \emph{catch-fire} semantics).
In contrast, atomic accesses can be used to access memory that can be shared between different threads. 
The results of data-races on these accesses are specified by the language and do not lead to undefined behavior.
The consequence is that some of the compiler optimization are not applicable to atomic accesses.
In addition to two kinds of memory accesses C/C++ also provide fences ---
low-level synchronization primitives similar to CPUs fence instructions.

\subsubsection{Java Memory Model}

The Java language makes different design choices and has different tradeoffs comparing to C/C++.
Unlike the later languages, Java provides safety and security guarantees
which are enforced both at compile time and at runtime.  
Thus Java language specification cannot tolerate undefined behaviors.
Despite that the Java compilers still make a lot of effort 
to provide good performance of the compiled code.

Consequently, Java memory model should follow this design 
and do not break any of the language's guarantees
while still admit fairly efficient compilation scheme
and allow as many optimizations as possible.

\subsection{Sound and efficient compilation scheme}

Having a short description of the memory models 
together with their design goals we are ready to proceed 
and consider whether these models satisfy the desired requirements.
We are going to start with compilation schemes.

As we have seen, the memory models of modern CPUs 
(those based on x86, ARMv8 and POWER architecture)
are weak and allow non sequentially consistent behaviors.
This is a result of various optimization implemented in hardware,
including instruction pipelines, speculative out of order executions, 
hierarchy of caches with complex coherence protocols, and others.

If the memory model wants to provide stronger guarantees 
than the CPU does (as for example sequentially consistent model)
it should somehow prevent the out of order executions.
In general, there are two ways to achieve that. 

First, as we have already discussed, special fence instructions,
provided by the CPU (such as \texttt{mfence} on x86) can be used.
These instructions usually flush caches, prevent speculative executions
and perform any other actions required by the hardware architecture
to forbid various weak behaviours.

Second, all modern hardware architectures do not reorder instructions 
if there are \emph{syntactic dependencies} between them. 
For example, the load instruction cannot be moved below 
the store instruction if there is conditional jump instruction between them.
The compiler can utilize this and prevent 
the reordering of the load instruction below subsequent stores
by emiting a usuless conditional jump instruction that would jump 
to the same label no matter what is the result of condition evaluation.
Dependencies of such kind can be computed following the 
syntax of the program (hence the name) as opposed 
to \emph{semantic dependencies} 
(we will see the difference between the two later).

Now that we have CPU fences and syntactic dependencies in our service
let's have a look at how they are used in real compilation schemes.

\subsubsection{Compiling SC}

Sequential consistency model is very expensive to implement in hardware. 
For this reasons all modern hardware architectures (including rather strong x86) do not provide it. 
We have seen this on the examples of Dekker lock and SB programs in \ref{introduction}.

In order to restore the sequential consistency on x86 one has to 
insert \texttt{mfence} instruction after each write.
On ARMv7 and POWER one need to insert full memory fence
(\texttt{dmb} and \texttt{sync} on ARMv7/POWER correspondingly)
before each write,
emit same full fence before each read, and also
add a syntactic dependency with special instruction fence 
(\texttt{isb} and \texttt{isync} on ARMv7/POWER correspondingly)
after the read.
Newer ARMv8 chips allows simpler solution, 
one just need to compile accesses to special 
load/store instruction to restore SC semantics
(\texttt{ldar} and \texttt{stlr}).
The table summarizes the resulting compilation mappings.

%% TODO: Here will be the table with compilation mappings

The natural question to ask is how much of the performance penalties
these compilation mappings induce compared to compiling all accesses as plain ones
without any fences or artificial syntactic dependencies.

%% TODO: cite some paper, present table with measurements etc. 

Thus one can see that enforcing sequential consistency on modern hardware is costly.

\subsubsection{Compiling C/C++}

\subsubsection{Compiling Java}

%% TODO: compiler optimizations --> program transformations (?)

\subsection{Soundness of compiler optimizations}

\subsubsection{Soundness of optimizations in SC}

\subsubsection{Soundness of optimizations in C/C++}

\subsubsection{Soundness of optimizations in Java}

\subsection{Reasoning}

\subsubsection{DRF Theorems}
