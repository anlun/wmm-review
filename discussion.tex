\section{Discussion}
\label{sec:discussion}

In this section we provide a summary of our findings.
We again briefly compare various classes of memory models 
and present a short guide for researchers and system-level developers 
on how to choose a memory model based on 
the design principles of a programming language.   

Clearly, the design of a memory model 
should reflect the design of a programming language itself. 

A language that seeks to provide a clear semantics and 
high-level programming abstractions, \eg \Haskell, at the cost 
of some performance losses most definitely should 
adhere to simple memory models like the sequential consistency. 

Programming languages focusing on efficiency 
of compiled code, for example, \CPP, 
have to resort to weak models admitting 
the optimal compilation mapping 
and wide range of various program transformations. 
For these languages it would be natural 
to pick some semantic dependency preserving model. 
However, there are a couple of subtle points here. 
First, models of this class are the most complicated, 
which makes reasoning about the programs' correctness challenging. 
Second, these models are still an active topic 
of the research and thus are subject to further modifications.

In the middle between two extremes are program order preserving and 
syntactic dependency preserving models.
Those are a reasonable choice for programming languages
which can afford a moderate performance overhead 
in exchange of a simpler and more predictable model~\cite{Ou-Demsky:OOPSLA18}.
% An example of such language is \OCaml --- 
% a high-level programming language 
% with an emphasis on functional paradigm which is 
% at the same time is actively used in performance sensitive areas
% like the development of compilers and verification tools. 

For languages adopting stronger models which require non-optimal
compilation mappings and forbid certain program transformations
there are some general optimization techniques and design decisions
which can partly mitigate the induced performance penalties.

A type system can serve as a great help in this task. 
Languages like \Haskell, \OCaml, \Rust that 
statically distinguish and isolate memory regions 
which can be accessed and modified concurrently have a great advantage.
These languages can identify precisely 
immutable and thread local variables
and compile accesses to them without insertion of fences.
Moreover, memory accesses of these classes are subject to 
a wide range of program transformations proven to be
sound for single threaded programs. 
 
Languages like \Java which cannot utilize the type system 
to fully prevent racy accesses to non-atomic variables 
because of the backward compatibility, still can 
approximate a set of thread local variables    
using a conservative static escape analysis~\cite{Choi-al:OOPSLA1999}
or various dynamic techniques~\cite{Liu-al:PLDI19},
and then apply similar optimizations to them. 

Functional programming languages encourage 
programmers to use immutable data whenever possible.
This style of programming minimizes the use 
shared memory and mitigates the performance impact 
of a strong memory model~\cite{Vollmer-al:PPoPP17}. 

Finally, if the language tolerates undefined behavior, as \CPP does, 
an alternative to a complex semantic dependency preserving model
could be a program order preserving model (or a syntactic dependency preserving one) 
which treats data races on non-atomic accesses as 
undefined behavior~\cite{Boehm-Demsky:MSPC14, Ou-Demsky:OOPSLA18}.
In this case a compiler can use optimal compilation mappings 
and apply a wide range of transformations to non-atomics 
and at the same time have a simpler semantics for atomics. 
