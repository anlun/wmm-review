\section{Discussion}
\label{sec:discussion}

In this section we provide a summary of our findings described in the previous sections. 
We again briefly compare various classes of memory models 
and present a short guide for the researchers and system-level developers 
on how to choose a memory model based on 
design principles of a programming language.   

Clearly, design of a memory model of a programming language
should reflect the design of the language itself. 

A language that seeks to provide clear semantics and 
high-level programming abstractions, \eg \Haskell, at the cost 
of some performance losses most definitely should 
adhere to simple memory models like sequential consistency. 

Programming languages focusing on efficiency 
of compiled code, for example, \CPP, 
have to resort to weak models admitting 
optimal compilation and wide range of 
various program transformations. 
For these languages it would be natural 
to pick some semantic dependency preserving model. 
However, there are a couple of subtle points here. 
First, models of these class are the most complicated, 
which makes reasoning about programs challenging. 
Second, these models are still an active topic 
of research and thus are subject to further modifications.

In the middle between two extremes are program order preserving and 
syntactic dependency preserving models.
Those are reasonable choice for programming languages
which can afford a moderate performance overhead 
in exchange of simpler and more predictable model~\cite{Ou-Demsky:OOPSLA18}.
% An example of such language is \OCaml --- 
% a high-level programming language 
% with an emphasis on functional paradigm which is 
% at the same time is actively used in performance sensitive areas
% like the development of compilers and verification tools. 

For languages adopting stronger models which require non-optimal
compilation mappings and forbid certain program transformations
there are some general optimization techniques and design decisions
which can partly mitigate induced performance penalties.

A type system can serve as a great help in this task. 
Languages like \Haskell, \OCaml, \Rust that 
statically distinguish and isolate memory regions 
which can be accessed and modified concurrently have a great advantage.
These languages can identify precisely 
immutable and thread local variables
and compile accesses to them without insertion of fences.
Moreover, memory accesses of these classes are subject to 
a wide range of program transformations proven to be
sound for single threaded programs. 
 
Languages like \Java which cannot utilize the type system 
to fully prevent racy accesses to non-atomic variables 
because of backward compatibility, still can 
approximate the set of thread local variables    
using conservative static escape analysis~\cite{Choi-al:OOPSLA1999}
or various dynamic techniques~\cite{Liu-al:PLDI19},
and then apply similar optimizations to them. 

Functional programming languages encourage 
the programmers to use immutable data whenever possible.
This style of programming minimizes the use 
shared memory and mitigates performance impact 
of strong memory model~\cite{Vollmer-al:PPoPP17}. 

Finally, if the language tolerates undefined behavior, as, \CPP, 
an alternative to complex semantic dependency preserving model
could be a program order preserving model (or syntactic dependency preserving one) 
which treats data races on non-atomic accesses as 
undefined behavior~\cite{Boehm-Demsky:MSPC14, Ou-Demsky:OOPSLA18}.
In this case the compiler can use optimal compilation mappings 
and wide range of transformations for non-atomics 
and at the same time have a simpler semantics for atomics. 
