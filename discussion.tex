\section{Discussion}
\label{sec:discussion}

In this section we provide a summary of our findings described in the previous sections. 
We again briefly compare various classes of memory models 
and present a short guide for the researchers and system-level developers 
on how to choose a memory model based 
on the design principles of the programming language.   
Finally, we highlight underdeveloped parts of the theoretical
and practical aspects of memory models, suggesting 
possible directions for future work in the field. 

\subsection{On the Choice of  Memory Model}

Clearly, design of the memory model of a programming language
should reflect the design of the language itself. 

A language that seeks to provide clear semantics and 
high-level programming abstractions, \eg \Haskell, at the cost 
of some performance losses most definetly should 
adhere to simple memory models like sequential consistency. 

Programming languages focusing on efficiency 
of compiled code, like, for example, \CPP, 
have to resort to weak models admitting 
optimal compilation and wide range of 
various program transformations. 
For these languages it would be natural 
to pick some semantic dependency preserving model. 
However, there are a couple of subtle points here. 
First, models of these class are quite complex, 
which makes reasoning about programs challenging. 
Second, these models are still an active topic 
of research and thus are subject to further modifications.

In the middle between two extremes are program order preserving and 
syntactic dependency preserving models.
Those are reasonable choice for programming languages
which can affort a moderate performance overhead 
in exchange of simpler and more predictable model~\cite{Ou-Demsky:OOPSLA18}.
An example of such language is \OCaml --- 
a high-level programming language 
with an emphasis on functional paradigm which is 
at the same time is actively used in performance sensitive areas
like the development of compilers and verification tools. 

For languages adopting strong models which require non-optimal
compilation mappings and forbid certain program transformations
there are some general optimization techiniques and design decisions
which can partly mitigate induced performance penalties.

A type system can serve as a great help in this task. 
Languages like \Haskell, \OCaml, \Rust that 
statically distinguish and isolate memory regions 
which can be accessed and modified concurrently have a great advantage.
These languages can identify precisely 
immutable and thread local variables
and compile accesses to them without insertion of fences.
Moreover memory accesses of these classes are subject to 
a wide range of program transformations proven to be
sound for single threaded programs. 
 
Languages like, for example, \Java or \Kotlin, which cannot utilize the type system 
to fully prevent racy accesses to non-atomic variables 
because of backward compatibility, still can 
use conservative static escape analysis~\cite{Choi-al:OOPSLA1999}
or various dynamic techinques~\cite{Liu-al:PLDI19} 
in order to approximate the set of thread local variables.   

Functional programming languages encourage 
the programmers to use immutable data whenever possible.
This style of programming minimizes the use 
shared memory and mitigates the performance impact 
of strong memory model~\cite{Vollmer-al:PPoPP17}. 

Finally, if the language tolerates undefined behavior, as, for example, \CPP, 
an alternative to complex semantic dependency preserving model
could be a program order preserving model (or syntactic dependency preserving one) 
which treats data races on non-atomic accesses as 
undefined behavior~\cite{Boehm-Demsky:MSPC14, Ou-Demsky:OOPSLA18}.
In this case the compiler can use optimal compilation mappings 
and wide range of transformations for non-atomics 
and at the same time have a simpler semantics for atomics. 

\subsection{Future Work}

Based on our analysis we can suggest several
possible direction for the future work. 

One can see that optimality of compilation schemes 
and local program transformations are relatively well studied.
More recent memory models~%
\cite{Lahav-al:PLDI17, Dolan-al:PLDI18, Kang-al:POPL17, Chakraborty-Vafeiadis:POPL19} 
support wide range of local transformations and have clear 
trade-offs in terms of compilation mappings. 
An exception is local transformations involving loops and recursion, 
their soundness have not being studied formally. 
Global transformations also received a little attention so far,
with a few notable exceptions~\cite{PichonPharabod-Sewell:POPL16, Lee-al:PLDI20}.
The exact impact of these transformations on the design of 
memory models is yet to be discovered. 

Whole program data-race freedom guarantees were also studied extensively.
In contrast, local data-race freedom is a relatively new concept. 
We expect it as well as local reasoning guarantees 
in general~\cite{Dodds-al:ESOP18, Jagadeesan-al:OOPSLA2020, Cho-al:PLDI21} 
to receive more attention in near future.  

In terms of features supported by memory models, 
we highlight the two challenging areas.
First, theory of mixed size accesses is clearly 
underdevoped to this day, only the two recent studies 
tried to approach it so far~\cite{Flur-al:POPL17, Watt-al:PLDI2020}.
Yet mixed size accesses are used in real-world applications,
for example, in the \Linux kernel codebase~\cite{Flur-al:POPL17},
and thus supporting them is a a necessary prerequisite 
for a certain class of memory models. 
Second, sequential consistent accesses, quite surprisingly,
posses a challenge for some of semantic dependency preserving
models like \Promising. Reconciling these models with $\sco$ 
accesses also would be a valuable contribution.

Semantic dependency preserving models are still an active 
area of research~\cite{Kang-al:POPL17, Lee-al:PLDI20, Cho-al:PLDI21,
Chakraborty-Vafeiadis:POPL19, Paviotti-al:ESOP20, 
Jagadeesan-al:OOPSLA2020}. 
We expect those to be a subject to 
further refinement and new proposals. 
An interesting line of work here would be 
a development of new reasoning principles
beyond data-race freedom, which could 
impove the meta-theory of these models and 
simplify the reasoning about correctness of programs.   

Finally, a comprehensive quantative studies 
of the performance penalties induced by memory models are quite valuable.
So far there were limited number of works in this direction~%
\cite{Singh-al:ISCA12, Liu-al:OOPSLA17, Liu-al:PLDI19, 
Vollmer-al:PPoPP17, Dolan-al:PLDI18, Ou-Demsky:OOPSLA18}, 
and the full picture is still unclear. 

