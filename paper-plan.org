#+TITLE: An Overview of Programming Language Memory Models
#+SUBTITLE: Plan of the research paper

* Abstract 
* Introduction
** Weak memory introduction

*** On data-races and usage of WMC

A main challenge in concurrent programming is 
to establish a proper synchronization between threads executed in parallel.     
Usually it is done with the help of synchronization primitives
provided by the programming language or libraries.
Examples of such primitives are locks, semaphores, barriers, channels and other.
If the programmer uses these primitives properly, he/she should not worry about concurrencly related bugs.

%% In this setting plain accesses to mutable shared variables are usually considered hazardous,

However, if the programmer want to use plain accesses to mutable shared variables, 
he/she must be very careful, because it can lead to data-races, a situation when two threads 
concurrently access the same variable and at least one of them is performing a write operation.
Data-races serve as a source of bugs in program,
which are hard to detect due to nondeterministic nature of concurrent programs.
     
Unfortunately data-races are unavoidable in some cases. 
For example, the implementation of synchronization primitives themselves usually contain data-races.
Another important example is lock-free data-structures that use fine grained synchronization 
in order to increase throughput of the program.
Thus it is important for a programming language to specify the behaviour of programs containing data-races.
This is what the memory model of programming language is responsible for.  

%% The memory model defines which values the reads of shared variables can observe at each point of the execution. 

Let us consider the consequences of having data-races in concurrent program on concrete example.

*** Here is SB program.

Consider a tiny program depicted below. 

%% Here will be a figure with SB program

In this program there are two threads, separated by two vertical bars, 
two shared variables ~x~ and ~y~, and two local variables ~r1~ and ~r2~, which we will also call registers.
In order to distinguish local and shared variables we enclose the latter into squre brackets, 
i.e. we write ~[x]~ and ~[y]~ to emphasise these variables are shared. 
Left thread first performs a write to ~[x]~ and then reads value from ~[y]~ into register ~r1~.
Right thread does the same but with ~[x]~ and ~[y]~ switched their roles.

%% Note the data races between write/read of ~[x]~/~[y]~.

*** We usually expect only SC semantics.

After running this program on multicore system, one would expect to see 
that either ~r1 = 0 /\ r2 = 1~, ~r1 = 1 /\ r2 = 0~, or ~r1 = 1 /\ r2 = 1~.
These behaviours can be explained as a result of interleaving of the instructions of parallel threads. 

**** Brief introduction of SC. Link to Lamport-TC79.

A memory model that admits only these behaviours is known under the name /sequential consistency/ (SC) [Lamport:TC79].

*** But it isn't in GCC+x86

Unexpectedly, if one will try to port this program to C language,
compile it with some compiler, say GCC, and run it on some real modern CPU, like x86, 
one can also observe the behaviour ~r1 = 0 /\ r2 = 0~.

*** Two reasons
**** Compiler optimizations
**** CPU optimizations

The last non-SC behaviour appears because of optimizations,
that could be performed either by the compiler or by the CPU. 
The optimizer can notice that write to ~[x]~ and read from ~[y]~
are independent instructions and thus they can be interchanged
(this optimization is perfectly valid for a single threaded program).
After the transformation the program looks as follows

%% Here will be a figure with transformed SB program

and now the behaviour with ~r1 = 0 /\ r2 = 0~ can be obtained 
even if it is executed assuming SC semantics.

The memory models that admit non-SC behaviours are called weak memory models
(correspondingly non-SC behaviours themselves are called weak behaviours). 

*** Dekker's lock is correct in SC, but not in weak memory.

The appearance of weak behaviours can lead to very devastating consequences.
For example a similar pattern to the SB program can be found in Dekker's algorithm for mutual exclusion.

%% Here will be a figure with Dekker's algorithm

Presence of weak behaviours breaks the correctness of the algorithm,
allowing two threads to enter critical section simultaneously. 

*** Fix w/ ~mfence~

In order to forbid weak behaviours and restore SC semantics
(and consequently restore correctness the Dekker's algoritm correct)
one has to use special annotations and CPU instructions, called memory fences,
in order to prevent compiler and CPU from reordering of the instructions. 
In case of x86 such instruction is called ~mfence~ and it prevents 
store to be moved below subsequent instructions.  

*** ~mfence~ solution has performance penalty

However, forbiding instruction reorderings on both the compiler's and CPU's level
has performance penalty and can slow down the program.  

** MMs in general
*** Informal memory model definition
**** Semantics of a concurrent system like CPU or programming language

*** Main tradeoff of MMs: simplicity (ease to work w/?) vs performance

Thereby, the main tradeoff of the memory model is its simplicity versus the performance penalty it induces. 

*** Different requirements and trade-offs for HW/PL

The memory models are usually split into two classes: 
models of hardware, that is modern CPU's like x86, ARMv8, POWER, etc, 
and models of programming languages, such as C/C++ or Java. 
Hardware and programming languages put different requirements 
on a memory model, that substantially effect its design and trade-offs.     


**** HW
***** Describe real CPUs
***** Room for future optimizations
***** Guarantees for PL MMs

The main requirement for the hardware memory models is that 
they should describe the behaviour of real modern CPU 
with all complex optimizations they made, 
like a hierarchy of memory caches, speculative executions, pipelining, etc.
Besides that the memory model should also leave some room for possible future optimizations.
Finally, it still needs to provide some reasonable guarantees for programs 
run on that CPU.       

**** PL

Programming language memory model features different set of requirements.

***** Compilation correctness to HW (link to SB example)

First, it should permit an efficient and sound compilation scheme to the modern hardware.
Efficient usually means that accesses to shared memory can be compiled 
without usage of memory fences, or with as little of them as possible. 
Soundness means that after the compilation the program when run on hardware 
(assuming memory model of some particular hardware) should not exhibit
any behaviours that were not allowed by the programming language memory models. 
Otherwise it would be very confusing for a programmer, 
because after the compilation he/she could observe the behaviours of the program 
which are forbidden according to the programming language semantics.

Going back to the SB example, one can conclude that SC is not really satisfies this criteria. 
In order to guarantee SC memory model for a programming language that should be able to be compiled into x86
one should issue the ~mfence~ instruction after each store to shared variable.
Compiled with such compilation scheme the program will run significantly slower 
than if it would have been compiled without memory fences.

***** Soundness of compiler optimizations (link to SB example)



Besides that the programming languge memory model should guarantee soundness of common compiler optimizations,
like for example reordering of independent instructions or common subexpression elimination.
It means that after an application of optimizations the program should not exibit any new behaviours.

Considering the SB example again, it can be seen that SC is not good with this respect too.
Reordering of independent instructions is not sound under SC, 
because, for example, after the reordering of the instructions of the left thread,
the new weak behaviour can be observed even if the program is compiled 
using the strictest memory fences offered by the hardware.  

***** Easy mode (DRF)

Contrary to the previous requirements, the memory model still should provide some reasonable guarantees.
For example, it should be possible for a programmer unfamiliar with subtleties of weak memory models 
to assume the SC model if he/she only uses corretly implemented syncronization primitives and data-structures
and never uses plain accesses to shared memory which could lead to data-races.
Guarantees of this kind are known as /Data-Race Free Theorems/ (DRF theorems) 
and usually they should be provided by any sane memory model.  

***** Reasoning and formal verification

Besides that it is very desirable for a memory model to be suitable for a formal reasoning and verification.
It implies that automated or semi-automated verification tools can be implemented 
in order to help the developers catch bugs in their concurrent code. 

%% As we will see, this requirement is not trivially satisfiable and in fact it does not met by many existing programming language memory models.   

***** ? UB and catch-fire semantics

** Existing problems w/ most popular PL MMs

*** Either
**** unsound compilation
**** inefficient compilation 
**** some common optimizations are unsound 
**** formal reasoning is impossible (memory model is too weak)

It turns out that none of the existing industrial specification of memory models 
for concurrent programming languages, like C/C++ or Java, really mets all of the requirements.
A memory model that can be efficiently compiled to the hardware, admits common compiler optimizations 
and at the same time provides strong enough guarantees for informal and formal reasoning
was a holy grail for the researchers in the field of formal semantics for a long time.
Only recently a major shift has been done, but even the most recently proposed solutions are flawed.    

** There are solutions w/ different trade-offs considered below

Thus the existing solutions have to make different trade-offs and either sacrifice performance or break some reasoning principles.  
In this paper we will consider several existing proposals for memory models of different programming languages,
discuss their design choices, limitations and ways to overcome them.  

** Paper structure 

The rest of the paper is organized as follows.
In section [1] we will discuss in more detail the requirements to the programming language memory models.
We will also look at specification of memory models for the C/C++ and Java languages
and see why these models do not meet the desired requirements.
In section [2] we will consider several proposed solutions to fix C/C++ MM. 
Section [3] contains an overview of memory models for JavaScript/WebAssembly and OCaml languages. 
Both of these models features some interesting properties that are currently lack in other models.
In section [4] we compare all of the memory models presented in the paper.
Finally, section [5] concludes with the discussion and open problems. 

* Requirements to Programming Language Memory Models (TODO: rework w.r.t. new introduction)
** Memory models under consideration
*** SC Memory Model 
**** "baseline" simple memory model
*** C/C++ Memory Model
**** should allow efficient compilation (zero-cost abstractions, don't pay for what you don't use, etc)
**** should allow agressive optimisations
**** can tolerate UB (Undefined Behaviour) in the semantics
*** Java Memory Model
**** should be as efficient as possible, yet
**** should be type and memory safe (no UB)
** Sound and efficient compilation scheme
*** General words about efficiency of compilation

We want efficient compilation to hardware.
Thus, relaxed accesses have to have as weak semantics as normal accesses on hardware.
However, sometimes it is necessary to have stronger accesses that prevent some intstruction reorderings.
Programming languages usually provide several types of accesses that compiled differently
(e.g. Java normal and ~volatile~ accesses, ~memory_order~ in C/C++)

*** Preventing instruction reorderings by hardware
There are several techniques which the compiler can use 
in order to prevent reorderings of intructions made by the processor  
**** fence instructions
**** intruction dependencies
*** Note on the cost of enforcing SC (compile everything with fences)  
*** Store buffering example (again)
**** explain example again
If relaxed accesses (~rlx~ in C/C++ or non-atomic in Java) 
are used in SB then after the compilation to x86 (or ARM/POWER)
the weak behaviors can appear. 
**** restoring sequential consistency
***** sc accesses (~sc~ in C/C++, ~volatile~ in Java)
***** sc accesses are compiled with ~mfence~ on x86 (mention ARM/POWER compilation?)
***** another way: using fences in PL (~atomic_thread_fence~ in C/C++)  
Discuss difference between sc acceeses and fences, 
perhaps it is better to do it in optimizations section. 
*** Message passing example
**** message passing program, weak behavior
**** introduce release/acquire accesses
***** difference with sc accesses  
Informal explanation: allow to 'syncronize' two threads in the program
but do not provide any 'global' syncronization.
Perhaps, illustrate this with IRIW example.
***** how they are compiled to hardware
****** plain accesses on x86, ~dmb~ on ARMv7, ~lda/stl~ on ARMv8, control dependency + ~isync~ on POWER 
*** Simlified spinlock example
**** introduce RMW (CAS, FAI, etc)  
**** splinlock implementation
**** note that usage of RMW and release/acquire accesses is important
**** how RMW are compiled
***** ~XCHG~ on x86
***** load-linked/store-conditional + loop on ARM/POWER
***** special instructions for FAI on ARMv8

*** Summary

There are several types of atomic accesses. 
Each of them should be compiled differently
in order to preserve the required guarantees
(e.g. to restore SC with sc atomics).
Atomic RMWs should be compiled using special hardware instructions
(either CAS-like or LL/SC + loop).
If we want the PL to be able to compile code in the most effcient way,
we need relaxed atomics that are compiled as plain loads/stores with no dependencies.    

** Soundness of compiler optimizations
*** General words about compiler optimizations
*** Local and global transformations
*** Fake dependencies elimination
**** LB examples. Real and fake dependencies. Semantics should be able to distinguish them. 
*** Example: unsound transformation in SC
**** reordering of independent memory accesses
*** Example: unsound transformation in JMM
**** redundunt read after read elimination
*** C/C++ is fine 
*** List of transformations that we might want to support (?)
** Reasoning
*** DRF (non-expert-mode)
**** DRF-SC in Java
***** example
**** DRF-SC in C/C++
***** OOTA problem
****** example
***** external/internal DRF
*** being suitable for formal verification techiniques
**** model checking 
***** a couple of words about model checking of SC
****** naive approach --- just enumerate all executions
****** mention that problem is decidable and NP-complete 
******* for programs without unbounded recursion and with finite domains
***** mention that checking whether JMM allows specific execution is undecidable
***** challenging (if possible?) for C/C++ because of OOTA
** UB and catch-fire semantics
*** Way to go for C/C++
*** Not an option for Java (safe language)
*** Opportunities for compilation and optimisations
** Summary
* Towards No-Thin-Air Memory Model
** Motivation
** RC11
*** Conservative approach
**** advantage --- simplicity
**** disadvantage --- performance penalty
***** compiler and hardware need to preserve load/store pairs (in other words cannot rearrange them)

****** relaxed loads should be compiled with fake dependency on ARM/POWER 
****** independent load/store reordering transformation is forbidden

***** Discuss the cost of performance penalty. Reference to [Ou-Demsky-OOPSLA18].
*** Reference to UB in the context of forcing po ∪ rf acyclicity
**** C++: only ~atomic~ accesses
**** Java: all accesses
*** A brief look at formal semantics
**** intoduce axiomatic/declarative semantics 
***** events, pre-execution graphs (traces), execution graphs, constraints (axioms) 
**** show examples on LB programs. 
*** Reasoning
**** DRF-SC is restored
**** efficient stateless model checking (cite [Kokologiannakis-et-al:POPL-17,Kokologiannakis-et-al:PLDI-19]) 

** Promising (1.0 and 2.0)
*** Idea --- allow causality (po ∪ rf) cycles that can be semantically certified 
**** consequences for compilation/optimizations --- no performance penalty
***** relaxed load/stores can be compiled as plain load/stores
***** reordering of independent load/stores is su
**** disadvantage --- model complexity
*** A brief look at formal semantics
**** operational semantics (abstract machine)
***** timestamps and viewfronts
***** promises and certification
**** show examples on LB programs
*** Local optimizations
*** Global optimizations
*** Reasoning
**** DRF-RA and DRF-SC

** Weakestmo
*** Motivation
**** same goal as Promising, but tries to solve some of its problems
***** being more declarative (easier to adapt/modify)
***** support for SC accesses
*** A brief look at formal semantics
**** introduce event structures
**** operational semantics for ES construction
**** show examples on LB programs
*** Reasoning
**** DRF-RLX (proof is broken) (?)
**** discuss model checking (not yet published) (?)
** Modular Relaxed Dependencies
*** Idea --- distinguish real and fake dependencies  
**** mention that semantics is ?denotational?
ANTON: only partially denotational. Their calculation of ``real'' dependencies denotational.
*** A brief look at formal semantics
**** show examples on LB programs
*** Reasoning
**** discuss challenges for model checking
** Summary comparing the solutions
*** Discuss challenges for model checking 
*** Supported memory access types (rlx, rel/acq, sc)
**** Promising doesn't support SC and it's hard to add there.
* Other Models and features
** JS/WASM Memory Model
*** introduce ~SharedArrayBuffer~
*** discuss mixed-size accesses
*** formal definition
**** examples (?)
*** compilation
*** optimisations

** OCaml Memory Model
*** intro (Multicore OCaml)
*** formal definition
**** axiomatic and operational version
*** compilation
*** optimisation
*** reasoning
**** local DRF
* Comparison
** Summary table
*** style: execution graphs, event structures, abstract machine
*** efficient compilation
*** compiler optimisations
*** DRF
*** UB
*** no OOTA
*** suitable for model checking
*** subjective complexity
** Summary table with compilation mappings (?)
** Summary table with supported optimisations (?)
** Summary table with performance overhead (?)
* Discussion and Open Problems
