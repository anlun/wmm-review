
#+TITLE: An Overview of Programming Language Memory Models
#+SUBTITLE: Plan of the research paper

#+INFOJS_OPT: view:showall toc:nil mouse:#cccccc ltoc:nil

* Abstract 
* Introduction
** Weak memory introduction

*** On data-races and usage of WMC

A main challenge in concurrent programming is 
to establish a proper synchronization between threads executed in parallel.     
Usually it is done with the help of synchronization primitives
provided by the programming language or libraries.
Examples of such primitives are locks, semaphores, barriers, channels and other.
If the programmer uses these primitives properly, she should not worry about concurrencly related bugs.
Otherwise she must be very careful. Accesses to mutable shared variables, 
that are not protected by locks or other syncronization primitives, can lead to data-races. 
Data-race is a situation when two threads concurrently access the same variable and at least one of them is performing a write operation.

%% Data-races serve as a source of bugs in program, which are hard to detect due to nondeterministic nature of concurrent programs.
     
Unfortunately data-races are unavoidable in some cases. 
For example, the implementation of synchronization primitives themselves usually contain data-races.
Another important example is lock-free data-structures that use fine grained synchronization in order to increase throughput of the program.
Thus it is important for a programming language to specify the behaviour of programs containing data-races.

This is what the memory model of programming language is responsible for.  
The memory model defines which values the reads of shared variables can observe at each point of the execution. 
In other words, it defines the semantics of concurrent program.
% (in the rest of the paper we will use terms memory model and program semantics interchangeably).

Let us consider the consequences of having data-races on a concrete example.

*** Dekker's lock is correct in SC, but not in weak memory.
    
Consider a variant of Dekker's algorithm for mutual exclusion.

%% Here will be a figure with Dekker's algorithm

In this program there are two threads accessing shared variables ~flag_i~ and ~turn~.
We enclose accesses to shared locations into square brackets, i.e., we write ~[flag_i]~ and ~[turn_i]~,
in order to distinguish them from accesses to registers (local variables).
Thread ~i~ use variable ~flag_i~ to indicate its intention to enter the critical section.
Variable ~turn~ is used to determine which thread can enter critical section next. 
 
%% Note the data races between write/read of ~[flag_i]~.

The algorithm heavily relies on the fact that both threads cannot simultaneously read value ~0~ at lines \ref{...}.
Otherwise the two threads would have been able to enter critical section simultaneously, thus breaking the correctness of the algorithm.
At first this assumption seems perfectly valid but let's a have a closer look.

*** Here is SB program.

Consider a program depicted below, which is a fragment of Dekker's algorithm.

%% Here will be a figure with SB program

*** We usually expect only SC semantics.

In this program there are two threads accessing shared locations ~[x]~ and ~[y]~ and 
storing values read from the locations to registers ~r1~ and ~r2~.
After running this program on a multi-core system, one would expect to see 
one of the following outcomes: ~{[r1=0, r2=1], [r1=1,r2=0], [r1=1,r2=1]}~.
These outcomes matches the assumptions of Dekker's algorithm.
Each one of them may be explained as a result of a sequential execution of an interleaving of threads' instructions. 

(ANTON: If we have enough space, I'd put a figure w/ the interleavings.)
(ANTON: IMO, we should more carefully distinguish terms "behavior" and "outcome".)
(ANTON: Also, we should state somewhere that, in the context of concurrent programs, we use terms "semantics" and "memory model" interchangeably.)

**** Brief introduction of SC. Link to Lamport-TC79.
A memory model that admits only these behaviours is known under the name /sequential consistency/ (SC) [Lamport:TC79].

*** But it isn't in GCC+x86

However, real concurrent systems may have different semantics. 
Thus, if one port this program to C language, compile it with the GCC compiler, and run on an x86 machine,
she may also observe the outcome ~[r1=0, r2=0]~.

*** Two reasons
**** Compiler optimizations
**** CPU optimizations

The last non-SC behaviour is called /weak/.
It can appears because of optimizations,
that could be performed either by the compiler or by the CPU. 
The optimizer may observe that write to ~[x]~ and read from ~[y]~
are independent instructions and thus they may be interchanged
(this optimization is perfectly valid for single-threaded programs).
After the transformation the program looks as follows

%% Here will be a figure with transformed SB program

for which the outcome ~[r1 = 0, r2 = 0]~ may be obtained under the SC semantics.

*** Fix w/ ~mfence~

In order to to prevent compiler and CPU from reordering of the instructions 
and thus forbid weak behaviours and restore SC semantics
(and consequently restore correctness the Dekker's algoritm correct)
one has to use special annotations and CPU instructions, called /memory fences/.
In case of x86 such instruction is called ~mfence~ and it prevents 
store to be moved below subsequent instructions.  

*** ~mfence~ solution has performance penalty

However, forbiding every possible instruction reordering  
on both the compiler's and CPU's level by putting memory fences everywhere
has performance penalty and can slow down the program.
Thus the programmer who want to implement a concurrent algorithm 
need to understand the memory model of the underlying system 
and put memory fences carefully.  

** Weak memory models 

As we have seen, the modern CPU's do not provide us sequentially consistent memory model by default. 
Memory models of programming languages also cannot provides us guarantees of sequential consistency
without a sacrifice of the compiled code's performance .  
Thus memory models of modern systems have to be /weak/, that is they have to allow weak outcomes.  

*** Main tradeoff of MMs: simplicity (ease to work w/?) vs performance

The main qustions is how weak the memory model should be?
The stronger model give more guarantees and thus is simpler to reason about while the weaker model permits more optimizatons. 
The main tradeoff of the memory model therefore is its simplicity versus the performance penalty it induces. 

*** Different requirements and trade-offs for HW/PL

The memory models are usually split into two classes: 
models of hardware, that is modern CPU's like x86, ARMv8, POWER, etc, 
and models of programming languages, such as C/C++ or Java. 
Hardware and programming languages put different requirements 
on a memory model, that substantially effect its design and trade-offs.     

**** HW
***** Describe real CPUs
***** Room for future optimizations
***** Preserve syntactic dependencies
***** Guarantees for PL MMs

The main requirement for the hardware memory models is that 
they should describe the behaviour of real modern CPU 
with all complex optimizations they made, like a hierarchy of memory caches, speculative executions, pipelining, etc.
Besides that the memory model should also leave some room for possible future optimizations.
Finally, it still needs to provide some reasonable guarantees for programs run on that CPU.       
(ANTON: IMO, this section deserves some details and explanations because we aren't going to return to HW models.
For example, more on future optimizations: there are behaviors allowed by models but not observed on CPUs [Alglave-al:TOPLAS14].
Maybe, here we should briefly say that HW models preserve dependencies to contrast it to PL models (w/o OOTA details for now).)

**** PL

A programming language memory model has different set of requirements.

***** Compilation correctness to HW (link to SB example)

First, it should permit an efficient and sound compilation scheme to the modern hardware.
Efficient usually means that accesses to shared memory can be compiled 
without usage of memory fences, or with as little of them as possible. 
Soundness means that after the compilation the program when run on hardware 
(assuming memory model of some particular hardware) should not exhibit
any behaviours that were not allowed by the programming language memory models. 

Going back to the SB example, one can conclude that SC is not really satisfies this criterion. 

Imagine if accesses to shared variables would have been compiled as plain load and store instructions of x86.
Then after running the compiled program one would be able to observe the weak outcome ~[r1 = 0, r2 = 0]~.
This outcome is not allowed by the SC semantics and yet it can be observed.
Thus one can conclude that this compilation scheme is unsound. 

Alternatively, in order to preserve the SC semantics one could 
issue ~mfence~ instruction after each store to shared variable.
This compilation scheme is sound but inefficient.
Programs compiled with such compilation scheme will run slower than 
if they would have been compiled without memory fences.

***** Soundness of compiler optimizations (link to SB example)

Besides that the programming languge memory model should guarantee soundness of common compiler optimizations,
like, for example, reordering of independent instructions or common subexpression elimination.
Soundness of an optimization means that after an application 
of optimizations the program should not exibit any new behaviors.

Considering the SB example again, it can be seen that SC is not good with this respect too.
The allowed outcomes of the SB program are ~{[r1=0, r2=1], [r1=1,r2=0], [r1=1,r2=1]}~.
After reordering of independent instructions in the left thread the program looks as follows.

%% here will be a picture of SB after instruction reordering

For this program SC model allows the following outcomes: 
~{[r1=0, r2=1], [r1=1,r2=0], [r1=1,r2=1], [r1=0,r2=0]}~.
Comparing them with the outcomes of the original program one can notice
that there is one new outcome ~[r1=0,r2=0]~.
This an evidence that reordering of independent instructions is not sound under SC.

***** Easy mode (DRF)

Contrary to the previous requirements, the memory model still should provide some reasonable guarantees.
(ANTON: For now, the contrast between the requirements is unclear. Maybe, at the end of the PL requirements section,
we should mention that the first two criteria push a memory model to be weaker, whereas the third one---to be stronger.)
(EVGENII: Currently there is a sentence about it above so perhaps it's fine now?)
For example, it should be possible for a programmer unfamiliar with subtleties of weak memory models 
to assume the SC model if she only uses correctly implemented synchronization primitives 
and data-structures and has no data-races in her program.
Guarantees of this kind are known as /Data-Race Free Theorems/ (DRF theorems) 
and usually they should be provided by any sane memory model.  

***** Reasoning and formal verification

Besides that it is very desirable for a memory model to be suitable for a formal reasoning and verification.
It implies that automated or semi-automated verification tools can be implemented 
in order to help the developers catch bugs in their concurrent code. 

%% As we will see, this requirement is not trivially satisfiable and in fact it does not met by many existing programming language memory models.   

***** ? UB and catch-fire semantics

** Existing problems w/ most popular PL MMs

*** Either
**** unsound compilation
**** inefficient compilation 
**** some common optimizations are unsound 
**** formal reasoning is impossible (memory model is too weak)

It turns out that none of the existing industrial specification of memory models 
for concurrent programming languages, like C/C++ or Java, really meets all of the requirements.
A memory model that can be efficiently compiled to the hardware, admits common compiler optimizations 
and at the same time provides strong enough guarantees for informal and formal reasoning
was an open research problem for quite some time. 
Only recently a major shift has been done, but even the 
newly proposed solutions has some drawbacks, limitations, and trade-offs.    

** There are solutions w/ different trade-offs considered below

The goal of this paper is to give a comprehensive review of 
the existing formal memory models for programming languages,
discuss their design choices, limitations and ways to overcome them.   

** Paper structure 
The rest of the paper is organized as follows.
In section [1] we will discuss in more detail the requirements to the programming language memory models.
On the way we will also look at specification of memory models for the C/C++ and Java languages
and see why these models do not meet the desired requirements.
In section [2] we will consider several proposed solutions to fix C/C++ MM. 
Section [3] contains an overview of memory models for JavaScript/WebAssembly and OCaml languages. 
Both of these models features some interesting properties that are currently lack in other models.
In section [4] we compare all of the memory models presented in the paper.
Finally, section [5] concludes with the discussion and open problems. 

* Requirements to Programming Language Memory Models

In this section we will have a closer look into the requirements 
that a programming language memory model should satisfy, 
namely the existence of sound and efficient compilation scheme, 
soundness of common optimizations, and guarantees for formal and informal reasoning.  
Taking as an examples three memory models: 
sequential consistency, java memory model as formalized in [Manson:05] 
and C/C++ memory model as formalized in [Batty:11]
(EVGENII: also cite corresponding java and c/c++ specs?),
we will see that each of them fails at least in one aspect.

** Memory models under consideration

We choose to consider these three memory models for the following reasongs.

*** SC Memory Model 
**** "baseline" simple memory model

Sequential consistency is included as a "baseline" memory model. 
It is simple, does not permit any counterintuitive behaviors,
and has clear formal specification.

*** C/C++ Memory Model
Memory model of the C/C++ follows the design principles of the language itself.

**** should allow efficient compilation and aggressive optimizations
**** can tolerate UB (Undefined Behaviour) in the semantics

C and C++ languages positions themselves as low-level programming languages
that provide zero-cost abstractions that generally do not put any performance penalties. 
In other words the abstraction that these languages provide 
should be compiled into efficient assembly code,
and leave the room for the aggressive optimizations.

The efficiency of the compiled code however comes with a certain cost.
The programmer should strictly obey the rules and conventions
established by the language's specification, 
otherwise program is said to have /undefined behavior/ (UB for short)

With respect to the memory model, the above means that C/C++ compiler should
compile accesses to shared variables as plain memory accesses of the CPU,
and provide to the programmer low-level syncronization primitives
that can be mapped directly to CPU instructions.
Besides that, the compiler should be able to perform 
as many optimizations as possible to the code containing shared accesses.
As we will see, not all of the optimizations that are sound 
for a single thread program reamins sound for concurrent programs that contain data races. 
(EVGENII: give some example? reordering of reads to the same location, as Anton suggested?) 

For these reasons C/C++ distinguish two kind of memory accesses.
Non-atomic accesses are can be used to perform read or write to memory that 
is owned exclusively by one thread at the time of the access.
Data-races on non-atomic accesses lead to undefined behavior for the whole program
(so called /catch-fire/ semantics).
In contrast, atomic accesses can be used to access memory that can be shared between different threads. 
The results of data-races on these accesses are specified by the language and do not lead to undefined behavior.
The consequence is that some of the compiler optimization are not applicable to atomic accesses.
In addition to two kinds of memory accesses C/C++ also provide fences ---
low-level synchronization primitives similar to CPUs fence instructions.

# For these reasons C/C++ distinguish two kind of shared memory accesses.
# Non-atomic accesses are can be used to perform read or write to memory that 
# is owned exclusively by one thread at the time of the access.
# These accesses compiled down to plain load or stores.
# In contrast, atomic accesses can be used to access shared memory. 
# Depending on the additional annotations (called ~memory_order~ in C/C++)
# atomic accesses are either compiled as plain accesses or with 
# additional CPU instructions that provide syncronization.
# C/C++ also provide fences that are compiled directly to CPU fence instructions. 
# Besides the difference in compilation scheme, 
# some of the optimizations that are applicable to non-atomic accesses
# are not sound when applied to atomic accesses. 
# (EVGENII: give some example? reordering of reads to the same location, as Anton suggested?) 

*** Java Memory Model
**** should be as efficient as possible, yet
**** should be type and memory safe (no UB)

The Java language makes different design choices and has different tradeoffs comparing to C/C++.
Unlike the later languages, Java provides safety and security guarantees
which are enforced both at compile time and at runtime.  
Thus Java language specification cannot tolerate undefined behaviors.
Despite that the Java compilers still make a lot of effort 
to provide good performance of the compiled code.

Consequently, Java memory model should follow this design 
and do not break any of the language's guarantees
while still admit fairly efficient compilation scheme
and allow as many optimizations as possible.

** Sound and efficient compilation scheme
*** General words about efficiency of compilation

We want efficient compilation to hardware.
Thus, relaxed accesses have to have as weak semantics as normal accesses on hardware.
However, sometimes it is necessary to have stronger accesses that prevent some intstruction reorderings.
Programming languages usually provide several types of accesses that compiled differently
(e.g. Java normal and ~volatile~ accesses, ~memory_order~ in C/C++)

*** Preventing instruction reorderings by hardware
There are several techniques which the compiler can use 
in order to prevent reorderings of intructions made by the processor  
**** fence instructions
**** intruction dependencies

*** Compiling SC
**** Every load/store should be compiled with fence/dependency
**** Table with compilation mapping
**** Note on the cost of enforcing SC (add a table with slowdown?)

*** Compiling C/C++
**** non-atomic and relaxed accesses are compiled as plain load/stores on all modern hardware
**** this can lead to undesired weak behaviors (consider Dekker and SB again) 
**** restoring sequential consistency
***** fences (~atomic_thread_fence~ with ~sc~ fence in C/C++, compiled as ~mfence~ on x86)
***** sc accesses 
****** also compiled with ~mfence~ on x86 (mention ARM/POWER compilation?)

**** implementing lock without Dekker algorithm
***** one can argue that Dekker algorithm is simply broken w.r.t. weak memory models
***** thus one can implement lock using atomic compare and swap instructions of the modern hardware
***** ~atomic_compare_exchange~ (compiled as ~(LOCK) XCHG~ on x86)
***** spinlock
***** on x86 spinlock is fine
***** however, on ARM and POWER it's still buggy
***** one can use ~sc~ fences/accesses but it adds overhead on x86  
***** C/C++ has release/acquire fences and accesses for this purpose (synchronization of two threads)
***** compilation mappings for release/acquire (no overhead on x86, cheaper than ~sc~ on ARM and POWER)

**** Summary

There are several types of atomic accesses. 
Each of them should be compiled differently
in order to preserve the required guarantees
(e.g. to restore SC with sc atomics).
Atomic RMWs should be compiled using special hardware instructions
(either CAS-like or LL/SC + loop).
If we want the PL to be able to compile code in the most effcient way,
we need relaxed atomics that are compiled as plain loads/stores with no dependencies.    

*** Compiling Java 
**** Java (v.5) has plain and volatile variables 
**** plain one compiled as plain load/stores 
**** volatile ~= ~sc~ in C/C++
** Soundness of compiler optimizations
*** General words about compiler optimizations
*** Local and global transformations
**** examples of local transformations
***** reordering of independent accesses
**** examples of global transformations
***** register promotion
*** Fake dependencies elimination
**** LB examples. Real and fake dependencies. Semantics should be able to distinguish them. 
*** Optimizations in SC
**** reordering of independent memory accesses is unsound in SC
*** Optimizations in JMM
**** redundunt read after read elimination is unsound in Java
*** Optimizations in C/C++ 
*** List of transformations that we might want to support (?)
** Reasoning
*** DRF (non-expert-mode)
**** General words about importance of DRF theorems
**** DRF-SC in Java
***** example
**** DRF-SC in C/C++
***** OOTA problem
****** example
***** external/internal DRF
*** being suitable for formal verification techiniques
**** model checking 
***** general words about model checking
****** mention that we assume bounded model checking
***** model checking SC
****** naive approach --- just enumerate all executions
****** (?) mention that problem is decidable and NP-complete 
******* for programs without unbounded recursion and with finite domains
***** model checking Java
****** mention that checking whether JMM allows specific execution is undecidable
***** model checking C/C++
****** challenging (if possible?) for C/C++ because of OOTA
** UB and catch-fire semantics
*** Way to go for C/C++
*** Not an option for Java (safe language)
*** Opportunities for compilation and optimisations
** Summary
* Towards No-Thin-Air Memory Model
** Motivation
** RC11
*** Conservative approach
**** advantage --- simplicity
**** disadvantage --- performance penalty
***** compiler and hardware need to preserve load/store pairs (in other words cannot rearrange them)

****** relaxed loads should be compiled with fake dependency on ARM/POWER 
****** independent load/store reordering transformation is forbidden

***** Discuss the cost of performance penalty. Reference to [Ou-Demsky-OOPSLA18].
*** Reference to UB in the context of forcing po ∪ rf acyclicity
**** C++: only ~atomic~ accesses
**** Java: all accesses
*** A brief look at formal semantics
**** intoduce axiomatic/declarative semantics 
***** events, pre-execution graphs (traces), execution graphs, constraints (axioms) 
**** show examples on LB programs. 
*** Reasoning
**** DRF-SC is restored
**** efficient stateless model checking (cite [Kokologiannakis-et-al:POPL-17,Kokologiannakis-et-al:PLDI-19]) 

** Promising (1.0 and 2.0)
*** Idea --- allow causality (po ∪ rf) cycles that can be semantically certified 
**** consequences for compilation/optimizations --- no performance penalty
***** relaxed load/stores can be compiled as plain load/stores
***** reordering of independent load/stores is su
**** disadvantage --- model complexity
*** A brief look at formal semantics
**** operational semantics (abstract machine)
***** timestamps and viewfronts
***** promises and certification
**** show examples on LB programs
*** Local optimizations
*** Global optimizations
*** Reasoning
**** DRF-RA and DRF-SC

** Weakestmo
*** Motivation
**** same goal as Promising, but tries to solve some of its problems
***** being more declarative (easier to adapt/modify)
***** support for SC accesses
*** A brief look at formal semantics
**** introduce event structures
**** operational semantics for ES construction
**** show examples on LB programs
*** Reasoning
**** DRF-RLX (proof is broken) (?)
**** discuss model checking (not yet published) (?)
** Modular Relaxed Dependencies
*** Idea --- distinguish real and fake dependencies  
**** mention that semantics is ?denotational?
ANTON: only partially denotational. Their calculation of ``real'' dependencies denotational.
*** A brief look at formal semantics
**** show examples on LB programs
*** Reasoning
**** discuss challenges for model checking
** Summary comparing the solutions
*** Discuss challenges for model checking 
*** Supported memory access types (rlx, rel/acq, sc)
**** Promising doesn't support SC and it's hard to add there.
* Other Models and features
** JS/WASM Memory Model
*** introduce ~SharedArrayBuffer~
*** discuss mixed-size accesses
*** formal definition
**** examples (?)
*** compilation
*** optimisations

** OCaml Memory Model
*** intro (Multicore OCaml)
*** formal definition
**** axiomatic and operational version
*** compilation
*** optimisation
*** reasoning
**** local DRF
* Comparison
** Summary table
*** style: execution graphs, event structures, abstract machine
*** efficient compilation
*** compiler optimisations
*** DRF
*** UB
*** no OOTA
*** suitable for model checking
*** subjective complexity
** Summary table with compilation mappings (?)
** Summary table with supported optimisations (?)
** Summary table with performance overhead (?)
* Discussion and Open Problems
