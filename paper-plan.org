#+TITLE: An Overview of Programming Language Memory Models
#+SUBTITLE: Plan of the research paper

* Abstract 
* Introduction
** Weak memory introduction

*** On data-races and usage of WMC

A main challenge in concurrent programming is 
to establish a proper synchronization between threads executed in parallel.     
Usually it is done with the help of synchronization primitives
provided by the programming language or libraries.
Examples of such primitives are locks, semaphores, barriers, channels and other.
If the programmer uses these primitives properly, he/she should not worry about concurrencly related bugs.

%% In this setting plain accesses to mutable shared variables are usually considered hazardous,

However, if the programmer want to use plain accesses to mutable shared variables, 
he/she must be very careful, because it can lead to data-races, a situation when two threads 
concurrently access the same variable and at least one of them is performing a write operation.
Data-races serve as a source of bugs in program,
which are hard to detect due to nondeterministic nature of concurrent programs.
     
Unfortunately data-races are unavoidable in some cases. 
For example, the implementation of synchronization primitives themselves usually contain data-races.
Another important example is lock-free data-structures that use fine grained synchronization 
in order to increase throughput of the program.
Thus it is important for a programming language to specify the behaviour of programs containing data-races.
This is what the memory model of programming language is responsible for.  

%% The memory model defines which values the reads of shared variables can observe at each point of the execution. 

Let us consider the consequences of having data-races in concurrent program on concrete example.

*** Here is SB program.

Consider a tiny program depicted below. 

%% Here will be a figure with SB program

In this program there are two threads, separated by two vertical bars, 
two shared variables ~x~ and ~y~, and two local variables ~r1~ and ~r2~, which we will also call registers.
In order to distinguish local and shared variables we enclose the latter into squre brackets, 
i.e. we write ~[x]~ and ~[y]~ to emphasise these variables are shared. 
Left thread first performs a write to ~[x]~ and then reads value from ~[y]~ into register ~r1~.
Right thread does the same but with ~[x]~ and ~[y]~ switched their roles.

%% Note the data races between write/read of ~[x]~/~[y]~.

*** We usually expect only SC semantics.

After running this program on multicore system, one would expect to see 
that either ~r1 = 0 /\ r2 = 1~, ~r1 = 1 /\ r2 = 0~, or ~r1 = 1 /\ r2 = 1~.
These behaviours can be explained as a result of interleaving of the instructions of parallel threads. 

**** Brief introduction of SC. Link to Lamport-TC79.

A memory model that admits only these behaviours is known under the name /sequential consistency/ (SC) [Lamport:TC79].

*** But it isn't in GCC+x86

Unexpectedly, if one will try to port this program to C language,
compile it with some compiler, say GCC, and run it on some real modern CPU, like x86, 
one can also observe the behaviour ~r1 = 0 /\ r2 = 0~.

*** Two reasons
**** Compiler optimizations
**** CPU optimizations

The last non-SC behaviour appears because of optimizations,
that could be performed either by the compiler or by the CPU. 
The optimizer can notice that write to ~[x]~ and read from ~[y]~
are independent instructions and thus they can be interchanged
(this optimization is perfectly valid for a single threaded program).
After the transformation the program looks as follows

%% Here will be a figure with transformed SB program

and now the behaviour with ~r1 = 0 /\ r2 = 0~ can be obtained 
even if it is executed assuming SC semantics.

The memory models that admit non-SC behaviours are called weak memory models
(correspondingly non-SC behaviours themselves are called weak behaviours). 

*** Dekker's lock is correct in SC, but not in weak memory.

The appearance of weak behaviours can lead to very devastating consequences.
For example a similar pattern to the SB program can be found in Dekker's algorithm for mutual exclusion.

%% Here will be a figure with Dekker's algorithm

Presence of weak behaviours breaks the correctness of the algorithm,
allowing two threads to enter critical section simultaneously. 

*** Fix w/ ~mfence~

In order to forbid weak behaviours and restore SC semantics
(and consequently restore correctness the Dekker's algoritm correct)
one has to use special annotations and CPU instructions, called memory fences,
in order to prevent compiler and CPU from reordering of the instructions. 
In case of x86 such instruction is called ~mfence~ and it prevents 
store to be moved below subsequent instructions.  

*** ~mfence~ solution has performance penalty

However, forbiding instruction reorderings on both the compiler's and CPU's level
has performance penalty and can slow down the program.  

** MMs in general
*** Informal memory model definition
**** Semantics of a concurrent system like CPU or programming language

*** Main tradeoff of MMs: simplicity (ease to work w/?) vs performance
*** Different requirements and trade-offs for HW/PL
**** HW
***** Describe real CPUs
***** Room for future optimizations
***** Guarantees for PL MMs
**** PL
***** Soundness of compiler optimizations (link to SB example)
***** Compilation correctness to HW (link to SB example)
***** Easy mode (DRF)
***** Reasoning and formal verification
***** ? UB and catch-fire semantics
** Existing problems w/ most popular PL MMs
*** Either
**** unsound compilation
**** inefficient compilation 
**** some common optimizations are unsound 
**** formal reasoning is impossible (memory model is too weak)
** There are solutions w/ different trade-offs considered below
** Paper structure 
* Requirements to Programming Language Memory Models (TODO: rework w.r.t. new introduction)
** Memory models under consideration
*** SC Memory Model 
**** "baseline" simple memory model
*** C/C++ Memory Model
**** should allow efficient compilation (zero-cost abstractions, don't pay for what you don't use, etc)
**** should allow agressive optimisations
**** can tolerate UB (Undefined Behaviour) in the semantics
*** Java Memory Model
**** should be as efficient as possible, yet
**** should be type and memory safe (no UB)
** Sound and efficient compilation scheme
*** General words about efficiency of compilation

We want efficient compilation to hardware.
Thus, relaxed accesses have to have as weak semantics as normal accesses on hardware.
However, sometimes it is necessary to have stronger accesses that prevent some intstruction reorderings.
Programming languages usually provide several types of accesses that compiled differently
(e.g. Java normal and ~volatile~ accesses, ~memory_order~ in C/C++)

*** Preventing instruction reorderings by hardware
There are several techniques which the compiler can use 
in order to prevent reorderings of intructions made by the processor  
**** fence instructions
**** intruction dependencies
*** Note on the cost of enforcing SC (compile everything with fences)  
*** Store buffering example (again)
**** explain example again
If relaxed accesses (~rlx~ in C/C++ or non-atomic in Java) 
are used in SB then after the compilation to x86 (or ARM/POWER)
the weak behaviors can appear. 
**** restoring sequential consistency
***** sc accesses (~sc~ in C/C++, ~volatile~ in Java)
***** sc accesses are compiled with ~mfence~ on x86 (mention ARM/POWER compilation?)
***** another way: using fences in PL (~atomic_thread_fence~ in C/C++)  
Discuss difference between sc acceeses and fences, 
perhaps it is better to do it in optimizations section. 
*** Message passing example
**** message passing program, weak behavior
**** introduce release/acquire accesses
***** difference with sc accesses  
Informal explanation: allow to 'syncronize' two threads in the program
but do not provide any 'global' syncronization.
Perhaps, illustrate this with IRIW example.
***** how they are compiled to hardware
****** plain accesses on x86, ~dmb~ on ARMv7, ~lda/stl~ on ARMv8, control dependency + ~isync~ on POWER 
*** Simlified spinlock example
**** introduce RMW (CAS, FAI, etc)  
**** splinlock implementation
**** note that usage of RMW and release/acquire accesses is important
**** how RMW are compiled
***** ~XCHG~ on x86
***** load-linked/store-conditional + loop on ARM/POWER
***** special instructions for FAI on ARMv8

*** Summary

There are several types of atomic accesses. 
Each of them should be compiled differently
in order to preserve the required guarantees
(e.g. to restore SC with sc atomics).
Atomic RMWs should be compiled using special hardware instructions
(either CAS-like or LL/SC + loop).
If we want the PL to be able to compile code in the most effcient way,
we need relaxed atomics that are compiled as plain loads/stores with no dependencies.    

** Soundness of compiler optimizations
*** General words about compiler optimizations
*** Local and global transformations
*** Fake dependencies elimination
**** LB examples. Real and fake dependencies. Semantics should be able to distinguish them. 
*** Example: unsound transformation in SC
**** reordering of independent memory accesses
*** Example: unsound transformation in JMM
**** redundunt read after read elimination
*** C/C++ is fine 
*** List of transformations that we might want to support (?)
** Reasoning
*** DRF (non-expert-mode)
**** DRF-SC in Java
***** example
**** DRF-SC in C/C++
***** OOTA problem
****** example
***** external/internal DRF
*** being suitable for formal verification techiniques
**** model checking 
***** a couple of words about model checking of SC
****** naive approach --- just enumerate all executions
****** mention that problem is decidable and NP-complete 
******* for programs without unbounded recursion and with finite domains
***** mention that checking whether JMM allows specific execution is undecidable
***** challenging (if possible?) for C/C++ because of OOTA
** UB and catch-fire semantics
*** Way to go for C/C++
*** Not an option for Java (safe language)
*** Opportunities for compilation and optimisations
** Summary
* Towards No-Thin-Air Memory Model
** Motivation
** RC11
*** Conservative approach
**** advantage --- simplicity
**** disadvantage --- performance penalty
***** compiler and hardware need to preserve load/store pairs (in other words cannot rearrange them)

****** relaxed loads should be compiled with fake dependency on ARM/POWER 
****** independent load/store reordering transformation is forbidden

***** Discuss the cost of performance penalty. Reference to [Ou-Demsky-OOPSLA18].
*** Reference to UB in the context of forcing po ∪ rf acyclicity
**** C++: only ~atomic~ accesses
**** Java: all accesses
*** A brief look at formal semantics
**** intoduce axiomatic/declarative semantics 
***** events, pre-execution graphs (traces), execution graphs, constraints (axioms) 
**** show examples on LB programs. 
*** Reasoning
**** DRF-SC is restored
**** efficient stateless model checking (cite [Kokologiannakis-et-al:POPL-17,Kokologiannakis-et-al:PLDI-19]) 

** Promising (1.0 and 2.0)
*** Idea --- allow causality (po ∪ rf) cycles that can be semantically certified 
**** consequences for compilation/optimizations --- no performance penalty
***** relaxed load/stores can be compiled as plain load/stores
***** reordering of independent load/stores is su
**** disadvantage --- model complexity
*** A brief look at formal semantics
**** operational semantics (abstract machine)
***** timestamps and viewfronts
***** promises and certification
**** show examples on LB programs
*** Local optimizations
*** Global optimizations
*** Reasoning
**** DRF-RA and DRF-SC

** Weakestmo
*** Motivation
**** same goal as Promising, but tries to solve some of its problems
***** being more declarative (easier to adapt/modify)
***** support for SC accesses
*** A brief look at formal semantics
**** introduce event structures
**** operational semantics for ES construction
**** show examples on LB programs
*** Reasoning
**** DRF-RLX (proof is broken) (?)
**** discuss model checking (not yet published) (?)
** Modular Relaxed Dependencies
*** Idea --- distinguish real and fake dependencies  
**** mention that semantics is ?denotational?
ANTON: only partially denotational. Their calculation of ``real'' dependencies denotational.
*** A brief look at formal semantics
**** show examples on LB programs
*** Reasoning
**** discuss challenges for model checking
** Summary comparing the solutions
*** Discuss challenges for model checking 
*** Supported memory access types (rlx, rel/acq, sc)
**** Promising doesn't support SC and it's hard to add there.
* Other Models and features
** JS/WASM Memory Model
*** introduce ~SharedArrayBuffer~
*** discuss mixed-size accesses
*** formal definition
**** examples (?)
*** compilation
*** optimisations

** OCaml Memory Model
*** intro (Multicore OCaml)
*** formal definition
**** axiomatic and operational version
*** compilation
*** optimisation
*** reasoning
**** local DRF
* Comparison
** Summary table
*** style: execution graphs, event structures, abstract machine
*** efficient compilation
*** compiler optimisations
*** DRF
*** UB
*** no OOTA
*** suitable for model checking
*** subjective complexity
** Summary table with compilation mappings (?)
** Summary table with supported optimisations (?)
** Summary table with performance overhead (?)
* Discussion and Open Problems
