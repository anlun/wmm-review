#+TITLE: An Overview of Programming Language Memory Models
#+SUBTITLE: Plan of the research paper

* Abstract 
* Introduction
** Weak memory introduction
*** On data-races and usage of WMC

Usually, programmers implementing concurrent programs should use 
synchronization primitives (like locks) provided by programming language or libraries.
Data-races (TODO: explain what it is) are considered as errors in this setting.
Thus, if you properly use locks, then you don't need to think about WMC.
However, data-races are unavoidable in some cases
(e.g. implementation of synchronization primitives themselfs, lock-free data-structures, etc).
Thus WMC is important at least for system engineers implementing the locks, lock-free data-structures, etc.

Let us consider the consequences of having data-races in concurrent program on concrete example.

*** Here is SB program.
*** We usually expect only SC semantics.
**** Brief introduction of SC. Link to Lamport-TC79.
*** That is, Dekker's lock is correct in SC.
*** But it isn't in GCC+x86
*** Two reasons
**** Compiler optimizations
**** CPU optimizations
*** Fix w/ ~mfence~
*** ~mfence~ solution has performance penalty

** MMs in general
*** Informal memory model definition
**** Semantics of a concurrent system like CPU or programming language

*** Main tradeoff of MMs: simplicity (ease to work w/?) vs performance
*** Different requirements and trade-offs for HW/PL
**** HW
***** Describe real CPUs
***** Room for future optimizations
***** Guarantees for PL MMs
**** PL
***** Soundness of compiler optimizations (link to SB example)
***** Compilation correctness to HW (link to SB example)
***** Easy mode (DRF)
***** Reasoning and formal verification
***** ? UB and catch-fire semantics
** Existing problems w/ most popular PL MMs
*** Either
**** unsound compilation
**** inefficient compilation 
**** some common optimizations are unsound 
**** formal reasoning is impossible (memory model is too weak)
** There are solutions w/ different trade-offs considered below
** Paper structure 
* Requirements to Programming Language Memory Models (TODO: rework w.r.t. new introduction)
** Memory models under consideration
*** SC Memory Model 
**** "baseline" simple memory model
*** C/C++ Memory Model
**** should allow efficient compilation (zero-cost abstractions, don't pay for what you don't use, etc)
**** should allow agressive optimisations
**** can tolerate UB (Undefined Behaviour) in the semantics
*** Java Memory Model
**** should be as efficient as possible, yet
**** should be type and memory safe (no UB)
** Sound and efficient compilation scheme
*** General words about efficiency of compilation

We want efficient compilation to hardware.
Thus, relaxed accesses have to have as weak semantics as normal accesses on hardware.
However, sometimes it is necessary to have stronger accesses that prevent some intstruction reorderings.
Programming languages usually provide several types of accesses that compiled differently
(e.g. Java normal and ~volatile~ accesses, ~memory_order~ in C/C++)

*** Preventing instruction reorderings by hardware
There are several techniques which the compiler can use 
in order to prevent reorderings of intructions made by the processor  
**** fence instructions
**** intruction dependencies
*** Note on the cost of enforcing SC (compile everything with fences)  
*** Store buffering example (again)
**** explain example again
If relaxed accesses (~rlx~ in C/C++ or non-atomic in Java) 
are used in SB then after the compilation to x86 (or ARM/POWER)
the weak behaviors can appear. 
**** restoring sequential consistency
***** sc accesses (~sc~ in C/C++, ~volatile~ in Java)
***** sc accesses are compiled with ~mfence~ on x86 (mention ARM/POWER compilation?)
***** another way: using fences in PL (~atomic_thread_fence~ in C/C++)  
Discuss difference between sc acceeses and fences, 
perhaps it is better to do it in optimizations section. 
*** Message passing example
**** message passing program, weak behavior
**** introduce release/acquire accesses
***** difference with sc accesses  
Informal explanation: allow to 'syncronize' two threads in the program
but do not provide any 'global' syncronization.
Perhaps, illustrate this with IRIW example.
***** how they are compiled to hardware
****** plain accesses on x86, ~dmb~ on ARMv7, ~lda/stl~ on ARMv8, control dependency + ~isync~ on POWER 
*** Simlified spinlock example
**** introduce RMW (CAS, FAI, etc)  
**** splinlock implementation
**** note that usage of RMW and release/acquire accesses is important
**** how RMW are compiled
***** ~XCHG~ on x86
***** load-linked/store-conditional + loop on ARM/POWER
***** special instructions for FAI on ARMv8

*** Summary

There are several types of atomic accesses. 
Each of them should be compiled differently
in order to preserve the required guarantees
(e.g. to restore SC with sc atomics).
Atomic RMWs should be compiled using special hardware instructions
(either CAS-like or LL/SC + loop).
If we want the PL to be able to compile code in the most effcient way,
we need relaxed atomics that are compiled as plain loads/stores with no dependencies.    

** Soundness of compiler optimizations
*** General words about compiler optimizations
*** Local and global transformations
*** Fake dependencies elimination
**** LB examples. Real and fake dependencies. Semantics should be able to distinguish them. 
*** Example: unsound transformation in SC
**** reordering of independent memory accesses
*** Example: unsound transformation in JMM
**** redundunt read after read elimination
*** C/C++ is fine 
*** List of transformations that we might want to support (?)
** Reasoning
*** DRF (non-expert-mode)
**** DRF-SC in Java
***** example
**** DRF-SC in C/C++
***** OOTA problem
****** example
***** external/internal DRF
*** being suitable for formal verification techiniques
**** model checking 
***** a couple of words about model checking of SC
****** naive approach --- just enumerate all executions
****** mention that problem is decidable and NP-complete 
******* for programs without unbounded recursion and with finite domains
***** mention that checking whether JMM allows specific execution is undecidable
***** challenging (if possible?) for C/C++ because of OOTA
** UB and catch-fire semantics
*** Way to go for C/C++
*** Not an option for Java (safe language)
*** Opportunities for compilation and optimisations
** Summary
* Towards No-Thin-Air Memory Model
** ?Motivation? ANTON: should be discussed in the previous section. Evgenii: I think we need to briefly state it again in the beginning. 
** RC11 
*** Conservative approach 
**** advantage --- simplicity
**** disadvantage --- performance penalty
***** compiler and hardware need to preserve load/store pairs (in other words cannot rearrange them)

****** relaxed loads should be compiled with fake dependency on ARM/POWER 
****** independent load/store reordering transformation is forbidden

***** Discuss the cost of performance penalty. Reference to [Ou-Demsky-OOPSLA18].
*** Reference to UB in the context of forcing po ∪ rf acyclicity
**** C++: only ~atomic~ accesses
**** Java: all accesses
*** A brief look at formal semantics 
**** intoduce axiomatic/declarative semantics 
***** events, pre-execution graphs (traces), execution graphs, constraints (axioms) 
**** show examples on LB programs. 
*** Reasoning
**** DRF-SC is restored
**** efficient stateless model checking (cite [Kokologiannakis-et-al:POPL-17,Kokologiannakis-et-al:PLDI-19]) 

** Promising (1.0 and 2.0)
*** Idea --- allow causality (po ∪ rf) cycles that can be semantically certified 
**** consequences for compilation/optimizations --- no performance penalty
***** relaxed load/stores can be compiled as plain load/stores
***** reordering of independent load/stores is su
**** disadvantage --- model complexity
*** A brief look at formal semantics
**** operational semantics (abstract machine)
***** timestamps and viewfronts
***** promises and certification
**** show examples on LB programs
*** Local optimizations
*** Global optimizations
*** Reasoning
**** DRF-RA and DRF-SC

** Weakestmo
*** Motivation  
**** same goal as Promising, but tries to solve some of its problems
***** being more declarative (easier to adapt/modify)
***** support for SC accesses
*** A brief look at formal semantics 
**** introduce event structures
**** operational semantics for ES construction
**** show examples on LB programs
*** Reasoning
**** DRF-RLX (proof is broken) (?)
**** discuss model checking (not yet published) (?)
** Modular Relaxed Dependencies
*** Idea --- distinguish real and fake dependencies  
**** mention that semantics is ?denotational?
ANTON: only partially denotational. Their calculation of ``real'' dependencies denotational.
*** A brief look at formal semantics
**** show examples on LB programs
*** Reasoning
**** discuss challenges for model checking
** Summary comparing the solutions
*** Discuss challenges for model checking 
*** Supported memory access types (rlx, rel/acq, sc)
**** Promising doesn't support SC and it's hard to add there.
* Other Models and features 
** JS/WASM Memory Model
*** introduce ~SharedArrayBuffer~
*** discuss mixed-size accesses
*** formal definition
**** examples (?)
*** compilation
*** optimisations

** OCaml Memory Model
*** intro (Multicore OCaml)
*** formal definition
**** axiomatic and operational version
*** compilation
*** optimisation
*** reasoning
**** local DRF
* Comparison
** Summary table
*** style: execution graphs, event structures, abstract machine
*** efficient compilation
*** compiler optimisations
*** DRF
*** UB
*** no OOTA
*** suitable for model checking
*** subjective complexity
** Summary table with compilation mappings (?)
** Summary table with supported optimisations (?)
** Summary table with performance overhead (?)
* Discussion and Open Problems
