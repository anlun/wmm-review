\section{Analysis}

In this section we present the detailed comparison 
of the considered programming language memory models. 
We summarize our findings in~\cref{table:summary}.
Each row of the table corresponds to a memory model, denoted by its abbreviation. 
Columns of the table correspond to the properties of the models discussed in~\cref{sec:background}.
We split the properties into several subgroups. 

The first group is devoted to optimality of compilation mappings
to target hardware architectures. In order to be concise, 
we chose the binary classification of optimality, 
that is, we classify the compilation scheme as either optimal or not,
in the following sense.
We chose the weakest possible access mode supported by the model
and consider the compilation scheme for the memory accesses annotated by this mode. 
For the memory models that treat racy non-atomic accesses
as undefined behavior, we consider the compilation mapping
for most relaxed access types the model provides.
This is because the catch-fire semantics for racy non-atomics 
trivially permits the most optimal compilation mappings (see~\cref{sec:bgrnd-ub}).
We say that compilation scheme is \emph{optimal} if the 
accesses annotated by the most relaxed mode 
can be compiled just as plain load and store instructions 
of the given hardware architecture (\ie without memory fences or extra dependencies). 

The second group is dedicated to soundness of various program transformations. 
The classification is also binary: a transformation is either sound or unsound 
in the given memory model (in a sense stated in~\cref{sec:bgrnd-opt-sound}).
Again, to be concise, we do not consider all the combinations 
of program tranformations and memory access modes. 
Instead, we consider the weakest possible accesses which have fully defined semantics. 
We further split the transformations into global and local as in~\cref{sec:bgrnd-opt-sound}.

The third group corresponds to reasoning principles guaranteed by the model. 
It includes the following properties. What kind of DRF guarantee the model provides.
(we distinguish the internal, external and local DRF theorems, see~\cref{sec:bgrnd-drf}).
Whether the model has undefined behaviors (see~\cref{sec:bgrnd-ub}).
Whether the model permits out-of-thin-air values (see~\cref{sec:bgrnd-oota}).

Finally, the last group enumerates the list of memory access modes 
and fences supported by the model, as well as whether the model 
supports read-modify-write operations, locks, and mixed-size accesses.

Driven by our analysis of the models' properties, we partition all models into five classes. 
\app{I think this is the place to name the classes and briefly introduce them.}
The models from the same class have similar compilation mappings, 
set of sound program transformations, and provided reasoning guarantees.
Our classes are ordered by the weakness of the memory models they consist of.  
The strongest models are located at the top rows of the table, 
while the weakest are at the bottom. 

We next discuss each class in more details
(note that the order is different from the one in the~\cref{table:summary}). 
\app{I think we should briefly explain why the order is different.}
We also give some insight on the relationship
between compilation scheme optimality, 
soundness of transformations and reasoning guarantees.
In particular, we explain why the support of some reasoning guarantees 
disables some program transformations and requires more heavyweight 
compilation mappings to hardware.

\subsection{Strong SC-like Models}

Sequential Consistency (\SC) is one of the most intuitive models of concurrency.
Under this model, one can represent the state of the system as 
a simple mapping from memory locations to their values. 
Then each outcome can be obtained by a sequential interleaved execution 
of thread's instructions. Each execution induces a total order 
on individual memory access operations consistent with per-thread program order. 

\SC renders many common transformations unsound, 
including all kind of instruction reorderings and 
common subexpression elimination~\cite{Marino-al:PLDI11, Sevcik-Aspinall:ECOOP08}.
The fact that instruction reorderings are forbidden 
makes the model expensive to implement on modern hardware
since even relatively strong hardware model of x86,
permits store/load reorderings.
Therefore, in order to preserve sequential consistency during compilation,
the compiler need to emit heavyweight memory fences between store and load instructions,
which makes compilation mappings far from being optimal.  

In terms of reasoning guarantees, however, SC is quite a pleasant model. 
It gives the DRF-SC and coherence (\eupp{s/coherence/SC-per-location?}) 
properties for free%
\footnote{The SC semantics assigns to programs only sequantially consistent
outcomes by definition, thus satisfying DRF-SC without any preconditions.
The coherence property is equivalent to sequential consistency per location.
The fact that in SC model any execution is sequeantially consistent implies trivially
that the accesses to each location are also sequeantially consistent.}
and it is naturally program order preserving.
\app{I don't get the last sub-sentence.}

The conceptual simplicity of SC have inspired many researchers 
to adopt it and to try to mitigate the induced performance 
penalty by some additional measures.
Marino et al~\cite{Marino-al:PLDI11, Singh-al:ISCA12} 
examined the performance penalties to ensure end-to-end SC
enforced by (1) modified SC-preserving version 
of LLVM compiler infrastructure and 
(2) a modified version of x86-TSO hardware. 
To mitigate the induced overhead the authors 
utilized the observation that hardware need to 
enforce SC only for memory accesses to shared mutable variables. 
To classify the memory regions as either thread-local,
shared immutable, or shared mutable they have used 
a combination of static compiler analysis and 
dynamic analysis powered by modified hardware. 
They evaluated their approach on a number of benchmarks
and reported performance overhead of 6.2\% on average 
and ~17\% in maximum, compared to stock LLVM compiler 
and regular x86 hardware. 

The SC-Haskell memory model~\cite{Vollmer-al:PPoPP17}
were inspired by the same idea of separation
between the thread-local and shared mutable memory. 
To safely distinguish between the two 
the authors utilized the powerful strong type system of Haskell. 
The consequence of this approach is that the 
programmers need to follow a stricter discipline 
in order to please the type checker. 
The authors modified the GHC to preserve SC 
and then run 1,279 benchmarks on x86-64 hardware
to measure the performance penalties.
They reported 0.4\% geometric mean slowdown,
and noticed that only 12 benchmarks experienced 
slowdown greater than 10\%.

The DRFx~\cite{Marino-al:PLDI10} is another 
SC preserving memory model. In this memory model
the runtime system is guaranteed to raise 
an exception if the program has data-races, 
and yeild only sequantially consistent outcomes otherwise.
In order to make the runtime data-race detection feasible 
in practice, the authors propose several modifications 
to existing hardware.
The authors claim that any sequentially valid optimization 
(\eg instruction reorderings or common subexpression elimination),
is \textbf{sound} in DRFx model.
The only limitation is that these transformations can only be performed
withing the bounds of compiler-designated program regions.
Importantly, any transformation that introduces 
speculative reads or writes is \textbf{unsound},
since speculative optimizations can bring
data-races into otherwise race-free programs.
The expected performance overhead of the model 
is reported to be 3.25\% on average
assuming the efficient implementation 
of data-race detection in hardware. 
(compared to stock compiler and x86 hardware). 

\app{As a reader, I'd expect to see the slowdown numbers in the table in some
unified manner.}

Given the results of performance evaluation above,
one can argue that the cost is worth the prize
of having simple SC-like model.
However, we point out that the two 
of the solutions above~\cite{Singh-al:ISCA12, Marino-al:PLDI10} 
require non-trivial modifications to the 
existing hardware and compilers.
In case of SC-Haskell~\cite{Vollmer-al:PPoPP17}, 
the programmers are obligated to follow 
a strict programming discipline enforced 
by the type system of the language.
In all research papers above \app{Very unclear reference.} the expirements 
of proposed solutions were mainly evaluated on x86 hardware, 
and the impact of enforcing SC on weaker hardware
(ARM, POWER) is less clear.
\app{I think that the current text dictates that we have to say smth stronger: not studied.
However, it is not true. Why don't we discuss Liu-al-PLDI19 here?}
Moreover, while the reported performance overhead 
is relatively small on average, 
it is more significant for particular 
kind of programs that heavily utilize shared 
mutable memory (like lock-free data structures).
\eupp{check the evaluation in the papers once again
to support this claim}

\subsection{Strong TSO-like Models}

The next class of PL memory models we consider 
was inspired by TSO~\cite{Sewell-al:CACM10} and PSO~\cite{Sparc:94} 
hardware models. In these models, threads usually 
are equipped with \emph{store buffers}.
All store operations go to these buffers before they 
propogate into the main memory.  
In essence, store buffers enable 
store/load reordering (in case of~TSO),
and store/load \& store/store reordegins (in case of~PSO).

The models based on store buffers idea 
\app{Something is wrong w/ phrasing}
can be compiled down to x86 hardware without any 
performance penalty, since x86 implements TSO model itself.
That is, the compilation mappings to x86 are optimal.
However, when compiled down to weaker hardware (e.g. ARM, POWER)
the compiler indeed needs to take additional measures 
to enforce TSO/PSO like memory model.
\eupp{Perhaps, we can cite some paper here?} 

The TSO/PSO models are weaker than SC, while 
they are still relatively strong.
The external DRF-SC and coherence still hold
and program order is preserved.

We are aware of two papers that propose to use TSO/PSO 
like models as PL level memory models.

Demange et al.~\cite{Demange-al:POPL13} presented 
the \emph{Buffered Memory Model} (or BMM in short)
as a candidate model for Java language.
Their motivation, however, stemmed not from the desire 
to fully replace the Java Memory Model, but rather 
from the goal to build a verified version of 
Java Virtual Machine (akin to CompCertTSO project).
By taking a relatively simple and yet pragmatic memory model
as a first target they hoped to made this task feasible. 
The authors proved soundness of several program transformations
(including store/load reordering, speculative load introduction,
and several others, see 
the~\cref{table:summary} for details%
\footnote{\eupp{If we'll decide to mention unsoundness of 
thread inlining w.r.t. TSO
then cite~\cite{Lahav-Vafeiadis:FM16} 
(since the paper itself doesn't mention this transformation)}})
and the external DRF-SC theorem. 
They also modified existing open-source implementation of 
JVM~\cite{Pizlo-al:ECCS10} to preserve BMM and 
reported only~1\% average overhead 
compared to original version of the virtual machine. 
Again, the authors used only x86 hardware in their 
experiments, and the performance penalties 
are expected to be more significant on weaker hardware.   

In~\cite{Boudol-al:POPL09}
\app{It is better to use surnames of the authors here. Also, we use FirstSurname-al notation for citations if there are more than two authors.}
the authors propose 
an approach to formal semantics of relaxed memory models 
based on the abstract machine with the main memory 
and the hierarchial structure of store buffers 
with stores to different locations possibly 
propagating to the main memory out-of-order
(similarly to PSO model).
The authors present a proof of DRF-SC theorem,
but do not provide an extensive study 
of program transformations' soundness.

\subsection{OOTA Models}

We next move on to the other end of the memory models' spectrum. 
We consider the class uniting the weakest models of our analysis.
These models enable efficient compilation mappings and 
many program transformations, but at the cost of 
introducing thin-air values (see \cref{sec:bgrnd-oota}).
Hence we name this class of models as Out-of-Thin-Air (OOTA) models. 
 
The main common disadvantage of OOTA models is that 
they lack many fundamental reasoning 
principles~\cite{Boehm-Demsky:MSPC14, Batty-al:ESOP15}:
type safety and security gurantess can be violated, 
compositional reasoning is impossible, and
the external DRF-SC property cannot be established. 
The odd consequences of thin-air values manifest 
itself best on the following classical example~\cite{Boehm-Demsky:MSPC14}: 

\begin{equation*}
\inarrII{
  \readInst{}{r_1}{x}      \\
  \kw{if} {(r_1)} ~\{      \\
  \quad\writeInst{}{y}{1}  \\
  \}
}{
  \readInst{}{r_2}{x}      \\
  \kw{if} {(r_2)} ~\{      \\
  \quad\writeInst{}{x}{1}  \\
  \}
}
\tag{LB+ctrl}\label{ex:lb+ctrl}
\end{equation*}

For the memory model admitting thin-air values 
(as \eg \CMM~\cite{Batty-al:POPL11}), 
the outcome $[r_1=1, r_2=1]$ is perfectly valid
(one can see that the program above is analogous 
to the \ref{ex:lb+data}, except it has 
control dependencies between instructions 
in place of data dependencies).
Not only this outcome is completely unintuitive,
but it also contradicts to the external DRF-SC guarantee.
Indeed, in SC model the program above has 
a single valid execution with the outcome $[r_1=0, r_2=0]$ 
and no data-races, thus under DRF-SC compliant model 
it should also has this sole outcome.  

The most notable member of the OOTA class is the \CMM model~\cite{Batty-al:POPL11}.
\app{The flow is broken here.}
The C and C++ languages are widely known as low-level languages 
for system programming which focus on the efficiency of compiled code. 
The main design principle of these languages is to provide
so-called zero-cost abstraction that should be compiled 
into efficient assembly code and leave the room 
for the aggressive optimizations. 
The main purpose of the \CMM model was to adhere 
to the same principles. The memory model 
was meant to provide efficient compilation mappings 
and as many transformation as possible.
It was later shown that the formal model actually 
fails in achieving this goal. 
Vafeiadis~\etal~\cite{Vafeiadis-al:POPL15} have shown
that many program transformation that deemed to be correct
are actually unsound according to the formal model. 
Batty~\etal~\cite{Batty-al:ESOP15} have shown that 
the model also fails to provide external DRF guarantee, 
and that it is ultimately not possible to provide this guarantee
at all within the style of the \CMM formal semantics.
%Yet the authors show that the internal DRF 
A lot of work~\cite{Batty-al:POPL11, Sarkar-al:PLDI12, Batty-al:POPL12, Batty-al:POPL16} 
was dedicated to prove soundness of optimal compilation mappings 
with respect to formal models of hardware, 
and there the results were mostly positive.
\eupp{should we mention that SC proof was broken and 
also cite RC11 here?}. 
Flur~\etal~\cite{Flur-al:POPL17} have extended the model 
to support mixed-size accesses.
Finally, Nienhuis~\etal~\cite{Nienhuis-al:OOPSLA16} presented 
a formal executable semantics in terms of an abstract machine, 
equivalent to the original \CMM model. 
\app{I don't like the story in this paragraph. We decided that a model
could be supported by many papers. That is, it makes wrong to say about 
flaws of the model which were fixed, \ie by \cite{Vafeiadis-al:POPL15,Lahav-al:PLDI17}.}

The JavaScript memory model, \JSMM, inherited \app{I think to use this word you need first to point out that
\JSMM is based on \CMM} some properties of \CMM.
Like the latter, it also has the problem of thin-air values
and thus can only provide internal DRF-SC gurantee. 
Contrary to the \CMM, \JS model gives a fully defined 
semantics to non-atomics 
(\ie no undefined behavior for racy non-atomic accesses).  
Also, unlike \CMM, where the main language primitive is 
individual mutable atomic variables, \JSMM uses 
the primitive of \texttt{SharedArrayBuffer},
that is a linear mutable byte buffer.
Thus the model naturally supports mixed-size accesses.

Crary and Sullivan~\cite{Crary-Sullivan:POPL15} proposed 
an alternative approach to relaxed shared memory concurrency.
Instead of deriving the ordering constraints from the annotations 
on memory accesses, they propose to directly specify 
the ordering between memory access in the source code. 
Their approach is highly generic and subsumes 
the traditional memory order annotation in the style of \CMM.
Their model is very weak and permits thin-air values. 
Still the authors prove the internal DRF theorem.
\app{``Still'' and usage of the present tense here confuses me.}

Saraswat~\etal~\cite{Saraswat-al:PPoPP07} presented the \RAO memory model
where relaxed behaviors are explained through the transformations 
over sequentially consistent execution.
Depending on the exact choice of transformations 
their model is either (1) permits thin-air values or 
(2) preserves external DRF-SC. 
Since the transformations (\eg reorderings, value range based, \etc)
are baked into the model, their soundness follows immediatelly.  

Boudol and Petri~\cite{Boudol-Petri:ESOP10} proposed a general 
framework to study the effects of speculative execution in
shared memory setting. 
They have also noticed that the external DRF doest not 
hold in the presence of unrestricted speculations, 
yet the internal DRF theorem still can be proven. 

\subsection{$\lPO\cup\lRF$ Acyclic Models}
\label{sec:porf-acyc}

Contrintuitive behavior of OOTA models, together with the fact that they break 
many important reasoning principles (external DRF-SC among them), 
has lead over the time to the consensus in the research community that these models 
are not suited well for the role of 
programming languages memory models~\cite{Boehm-Demsky:MSPC14, Batty-al:ESOP15}.
A lot of effort has been put to forbid problematic 
thin-air outcomes, while still keep compilation scheme as efficient as possible
and enable as many transformations as possible.

The most straightforward way to forbid thin-air values 
was proposed by Boehm and Demsky~\cite{Boehm-Demsky:MSPC14}
The idea is to simply prohibit any kind of speculative execution, 
which is equivalent to forbidding load/store reorderings altogether. 
This fix not only restores external DRF-SC~\cite{Lahav-al:PLDI17}
and other reasoning guarantees, but also leads to 
a much simpler mental model.  

Lahav~\etal~\cite{Lahav-al:PLDI17} formalized this approach and 
studied it extensively. They proposed a modified version 
of \CPP model called \RCMM (repaired \CMM).  
Besides the strengthening of the model to preserve 
the order between load/store pairs, 
the repaired version also corrects the semantics 
of sequentially-consistent accesses.

The authors have shown that many 
program transformations are still sound in \RCMM, 
with the obvious exception of load/store reordering itself
(see~\cref{table:summary} for details).
Also, the compilation mappings to x86 remain efficient, 
since the architecture already guarantee to preserve the order 
between loads and subsequent stores. 
However, weaker architectures (ARM, POWER) do not guarantee that, 
and thus additional measures are required.
Lahav~\etal~\cite{Lahav-al:PLDI17} proposed to compile relaxed load 
as plain load followed by a spurious conditional branch,
which introduces fake control dependency between 
the load and subsequent stores. 
ARM and POWER hardware preserves dependencies, 
and thus it has to also retain the load/store ordering. 

Ou and Demsky~\cite{Ou-Demsky:OOPSLA18} have studied 
the performance penalty needed to guarantee 
\RCMM memory model on ARMv8 hardware.
They modified the LLVM compiler framework 
to enforce \RCMM memory model
by (1) adjusting the compiler optimization passes and 
(2) changing the compilation mappings.
Several compilation schemes were considered,
among them the one that uses spurious conditional branch
as descibed above has demonstrated the most promising results.  
The authors measured the running time on a set of benchmarks 
implementing various concurrent data-structures
and reported an overhead of 0\% on average and 6.3\% in maximum,
compared to the unmodified version of the compiler. 

A plenty of memory models have utilized the \RCMM
solution to forbid thin-air values by 
preserving the order between load/store pairs. 

Dang~\etal~\cite{Dang-al:POPL19} developed an operational 
version of \RCMM in terms of the abstract machine, 
which they called \ORCMM. Their motivation was to 
then develop a new program logic and show it's soundness
with respect to \ORCMM memory model. 
The program logic itself was then utilized to 
prove correctness of some parts of 
the \Rust~\cite{RustBook:19} standard library.
\eupp{Maybe we can omit this paper, since it's mostly about program logics?} 

Dolan~\etal~\cite{Dolan-al:PLDI18} developed a new 
memory model for the \MOCaml project. 
They were the first to propose the local DRF property. 
The authors also hinted that the local DRF property 
is not compatible with load/store reordering.
This fact forced them to forbid this transformation
and adapt similar compilation scheme as for \RCMM. 

An important divergence of \OCaml memory model 
from \CMM-like models is that the former 
has a weaker notion of coherence.
The choice of the weaker coherence was deliberate 
with the purpose to enable common subexpression elimination (CSE).
\eupp{Consider to move the details on Coherence vs. CSE
into separate subsection and then leave the reference here}.
The subtle effect of the strong coherence property 
on this classic compiler optimization was first 
observed in the context of \Java 
memory model~\cite{Pugh:JAVA99}.
To see the problem, consider the program below
(on the left) and the transformed version 
of this program after application of CSE (on the right).
Note that the optimization has replaced 
the second access to variable \texttt{x}
by a read from register. 

\begin{minipage}{0.45\linewidth}
\begin{equation*}
\small
\inarrII{
  \readInst{}{r_1}{x}      \\
  \readInst{}{r_2}{y}      \\
  \readInst{}{r_3}{x}      \\
}{
  \writeInst{}{y}{1}       \\
}
\label{ex:coh-rr}
\end{equation*}
\end{minipage}\hfill%
\begin{minipage}{0.05\linewidth}
\Large~\\ $\leadsto$
\end{minipage}\hfill%
\begin{minipage}{0.45\linewidth}
\begin{equation*}
\small
\inarrII{
  \readInst{}{r_1}{x}      \\
  \readInst{}{r_2}{y}      \\
  \assignInst{r_3}{r_1}    \\
}{
  \writeInst{}{y}{1}       \\
}
\label{ex:coh-rr}
\end{equation*}
\end{minipage}

Now assume that \texttt{x} and \texttt{y} point to the same memory location.
Under this assumption the outcome $[r_1=0, r_2=1, r_3=0]$
is forbidden for the memory model respecting coherence.
Indeed, the coherence guarantees sequential consistency per location, 
which means that for the programs consisting of accesses 
to the single memory location 
(as the one above in the presence of aliasing) 
only the sequentially consistent outcomes are allowed.
The outcome $[r_1=0, r_2=1, r_3=0]$ cannot be obtained 
by the interleaving of instructions, and thus 
it should be forbidden.  
However, this outcome is allowed for 
the optimized version of the program. 

Bender and Palsberg~\cite{Bender-Palsberg:OOPSLA19} formalized a new revision 
of the Java Memory Model~\cite{JDK9-VarHandle, JEP:193, JDK9-Modes}, 
which was developed to overcome 
the difficulties of the previous one~\cite{Manson-al:POPL05}
(see \ref{sec:prm-cert} for details).
The new version of the model was inspired by \RCMM. 
It introduced a system of annotations on memory accesses, 
called ``Java access modes'' (hence the name of the model --- \JAM),
similar to those present in \CMM like models.
The new model adopted the \RCMM solution to OOTA problem. 
It forbids load/store reorderings on the level of 
opaque (an analog of \CPP relaxed) or stronger accesses.
The model does not tackle the problem of 
thin-air values on the level of plain (\ie non-atomic) accesses.

Dodds~\etal~\cite{Dodds-al:ESOP18} proposed a denotational 
compositional semantics for the fragment of \CMM memory model, 
including non-atomic accesses with cath-fire semantics, 
release-acquire accesses, and sequantially-consistent fences. 
Based on this semantics the authors developed 
a tool for automatic verification of program transformations
in the considered fragment of the \CMM model. 
Since the relaxed fragment was not included, 
the authors avoided problems with thin-air values. 

\subsection{$\lPPO\cup\lRF$ Acyclic Models}
\label{sec:pporf-acyc}

An alternative conceptually simple solution 
to thin-air values problem is to preserve 
\emph{syntactic dependencies}~\cite{Boehm-Demsky:MSPC14, Alglave-al:ASPLOS18}.
Under this approach reordering of independent load/store pairs is not forbidden.
However, the reordering is forbidden if the store depends on the value 
read by the load either because this value 
was used to compute the value written by the store (\emph{data dependency}), 
or the memory address used in the store (\emph{address dependency}),
or else the control-flow path lead to the store was dependent
on this value (\emph{control dependency}).
For example, giving the program \ref{ex:lb+data} 
the store $\writeInst{}{y}{r_1}$ depends 
on the load $\writeInst{}{x}{r_1}$ since 
it writes the value read by the load.

Note that these kind of dependencies are computed following the 
syntax of the program (hence the name) as opposed 
to \emph{semantic dependencies}.
For example, giving the modified version of the \ref{ex:lb+data} program below, 
the store to \texttt{y} is still considered to be dependent on the previous load. 

\begin{equation*}
\inarrII{
  \readInst{}{r_1}{x}           \\
  \writeInst{}{y}{1 + 0 * r_1}  \\
}{
  \readInst{}{r_2}{x}      \\
  \writeInst{}{x}{r_2}     \\
}
\tag{LB+fakedata}\label{ex:lb+fakedata}
\end{equation*}

Here the syntactic dependency can be eliminated 
by the \emph{constant folding} transformation --- 
the expression $1 + 0 * r_1$ can be reduced to just~$1$.
Under the syntactic dependency preserving memory model 
the compiler, however, is prohibited to perform this optimization. 
Indeed, once the dependency is removed, nothing prevents 
to reorder the store before the preceding load. 
Even if the compiler itself does not perform this reordering,
after the compilation the hardware can do this during the execution.   

This subtlety reveals the main drawback of 
syntactic dependency tracking models --- 
various trace preserving transformations
(\eg constant folding) are unsound in these models. 
Constant folding is one of the basic kind of optimizations that 
any compiler might want to apply, 
and the fact that it is unsound  
makes the adoption of this class of models problematic.

Note that harware models apply a similar approach 
and usually have a notion of dependencies between 
the memory operations~\cite{Sarkar-al:PLDI11, Alglave-al:TOPLAS14, Pulte-al:POPL18}.
Yet in this setting the unsoundness of 
trace preserving transformation is not a problem,
since the hardware does not perform such complex optimizations.

The Linux kernel memory model \LKMM~\cite{Alglave-al:ASPLOS18} has adopted 
the idea to track syntactic dependencies in order to 
forbid thin-air values. In the context of OS kernel development 
this choice is justified. First, the Linux kernel targets 
a wide range of hardware architectures with a diverse
set of memory models. To simplify the reasoning about the code, 
it is reasonable to pick a model which is conceptually close
to those of hardware. Second, the kernel developers 
already utilze various techiniques to prevent 
certain compiler optimizations%
\cite{Alglave-al:ASPLOS18, LK-MemBarriers, LK-RCU-Deref}.

In attempt to repair Java memory model Zhang and Feng have proposed the 
operational happens-before model \OHMM~\cite{Zhang-Feng:FCS16}.
Their abstract machine consists of global event buffer,
where the events might be reordered before they propogate into  
a global history based memory, and a replay mechanism 
used to simulate speculative executions. 
To avoid thin-air outcomes the model tracks syntactic dependencies 
between the events and forbids the reordering of dependent events. 

Ou and Demsky~\cite{Ou-Demsky:OOPSLA18} also studied 
the performance penalty induced by dependency preserving compiler. 
Again, they modified the LLVM compiler infrastructure 
and run a number of benchmarks on ARMv8 hardware. 
They have observed 3.1\% overhead on average and 17.6\% in maximum. 

\subsection{no-OOTA Models}
\label{sec:prm-cert}

The last approach to tackle thin-air problem is to   
construct a notion of \emph{semantic dependencies}, 
which would precisely characterize what load/store 
pairs are ``independent'' and rule out 
``fake'' dependencies like the one in \ref{ex:lb+fakedata}.
It turns out that this task is challenging 
and to this date there is no strong consensus on how to achieve it.

The practical payoff of this approach is that it 
does not require significant modifications to existing compilers or hardware, 
and thus should not impose performance penalties.  
The ultimate goal is to enable optimal compilation mappings, 
preserve most of the existing compiler optimizations, 
and at the same time maintain the important 
reasoning guarantees like external DRF. 

The original formalized version of Java memory model \JMM~\cite{Manson-al:POPL05}
had taken this approach. While the most of the memory model 
was formalized in axiomatic style, it also used 
an operational notion of \emph{commit sequence}, 
\ie a sequence of partial execution graphs, to forbid thin-air outcomes. 
The model was shown to adhere external DRF~\cite{Huisman-Petri:CONCUR07}.
However the model failed to justify some program transformations 
which were expected to be sound~\cite{Sevcik-Aspinall:ECOOP08} 
(\eg redundunt load after load elimination, roach motel reodering, and others,
see \cref{table:summary} for details). 

The most evolved model of this class is 
\Promising operational semantics~\cite{Kang-al:POPL17, Lee-al:PLDI20}. 
Its key ingredient is the promising and certification machinery.
During the execution, the abstract machine can 
non-deterministically \emph{promise} to perform some store,
it then has to \emph{certify} the promise is feasible. 
The certification mechanism is defined in the way to forbid thin-air values to appear.
The authors of the model have proven formally 
that \Promising semantics admits many local and global program transformations
(see \cref{table:summary} for details).
Among the set of transformations usally considered 
in the relaxed memory literature, 
only the thread inlining was shown to be unsound%
\footnote{\CSE is left unsound intentionally due to coherence property}. 
The external DRF theorem was also shown to hold. 
Podkopaev~\etal~\cite{Podkopaev-al:ECOOP17, Podkopaev-al:POPL19} have proven formally
soundness of standard optimal compilation mappings to x86, POWER, ARMv7, and ARMv8.
One of a few limitations of the \Promising semantics is that 
it does not support sequentially consistent accesses. 

Jagadeesan~\etal~\cite{Jagadeesan-al:ESOP10} attempted to fix \JMM 
and proposed an operational semantics with speculative execution.
To avoid thin-air values they have put stratification constraints 
on speculations. The authors prove the external DRF theorem. 
Also they verified a few program transformations 
(store/store reordering, load/load elimination, and roach motel reordering), 
but overall their study of transformations was not systematic.  

Several authors proposed to use \emph{event structures}~\cite{Winskel:86}
to resolve thin-air problem. Event structures allow to represent 
multiple conflicting executions of the program in one mathematical structure.  

Jeffrey and Riely~\cite{Jeffrey-Riely:LICS16} proposed 
a memory model based on event structures and a notion of 
\emph{well-justification} of events inspired by the game theory. 
Well-justification is used to prevent thin-air values 
and prove external DRF. The authors do not study 
soundness of program transformations in their model. 
They show, however, a counterexample demonstrating that 
load/load reordering is unsound. It implies that 
the compilation mappings to ARMv7, ARMv8, and POWER 
are not optimal too.   

Pichon-Pharabod and Sewell~\cite{PichonPharabod-Sewell:POPL16} 
presented an operational memory model consisting of 
memory subsystem inspired by POWER and a thread subsystem, 
where each thread is represented as an event structure. 
At each step the abstract machine is allowed to either 
commit an event to the storage, or perform a transformation 
on one of the event structures. 
The authors have shown soundness of 
optimal compilation mappings to x86 and POWER, 
as well as soundness of several program transformations.
It was later revealed though that the compilation scheme
to ARMv7 and ARMv8 is not optimal~\cite{PichonPharabod:PhD18}.

Chakraborty and Vafeiadis~\cite{Chakraborty-Vafeiadis:POPL19}
also developed a memory model based on event structures. 
They utilize the event structures' capability to simultaneously encode 
multiple conflicting executions in order to model speculative executions.
Their model is close in spirit to conventional axiomatic models, 
however, instead of idividual execution graphs, they use 
the event structures upon which they put additional constraints. 
The model was shown to admit optimal compilation mappings~\cite{Moiseenko-al:ECOOP20},
several program transformation, and external DRF.
Unlike \Promising semantics, it also supports 
sequantially consistent accesses.

Paviotti~\etal~\cite{Paviotti-al:ESOP20} constructed a 
denotational semantics based on event structures. 
They employ the event structures to capture 
semantic dependencies between the memory access events, 
which are in ture used to rule out thin-air outcomes.
The authors prove the external DRF-SC and 
soundness of optimal compilation mappings,
also they present a refinement relation which 
can be used to reason about validity of program transformations. 

% \begin{itemize}
%   \item Pros.
%   \begin{itemize}
%     \item Cheap compilation to hardware.
%     \item Almost all transformations are sound (including all reorderings). 
%   \end{itemize}
%   \item Cons.
%   \begin{itemize}
%     \item Complexity.
%     \item No common ground between various models.
%     \item It's unknown how to make thread inlining sound (?).
%   \end{itemize}
% \end{itemize}

\newpage
\onecolumn

\begin{landscape}

\begin{table*}
\begin{tabular}{|c|l|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|}
 \hline

                                                      &
 \multirow{3}{*}{Model}                               & 
 \multicolumn{ 4}{c|}{\multirow{2}{*}{Compilation}}   &
 \multicolumn{10}{c|}{Transformations}                &
 \multicolumn{ 6}{c|}{Reasoning}                      &
 \multicolumn{ 9}{c|}{\multirow{2}{*}{Features}}      \\ 

 \cline{7-22}

                             &
                             &
 \multicolumn{4}{c|}{}       &
 \multicolumn{7}{c|}{Local}  &
 \multicolumn{3}{c|}{Global} &

 \multicolumn{3}{c|}{DRF}    &
 \multicolumn{3}{c|}{}       &
 \multicolumn{9}{c|}{}       \\ 
 
 \hline
                                     &
                                     &
 \rotatebox[origin=c]{270}{x86}      & 
 \rotatebox[origin=c]{270}{Power}    & 
 \rotatebox[origin=c]{270}{ARMv7}    & 
 \rotatebox[origin=c]{270}{ARMv8}    & 
 
 \rotatebox[origin=c]{270}{TP}     &
 \rotatebox[origin=c]{270}{RI}     &
 \rotatebox[origin=c]{270}{RE}     &
 \rotatebox[origin=c]{270}{ILE}    &
 \rotatebox[origin=c]{270}{SLI}    &
 \rotatebox[origin=c]{270}{S}      &
 \rotatebox[origin=c]{270}{RM}     &
 \rotatebox[origin=c]{270}{RP}     &
 \rotatebox[origin=c]{270}{VR}     &
 \rotatebox[origin=c]{270}{TI}     &
 
 \rotatebox[origin=c]{270}{Int}    &
 \rotatebox[origin=c]{270}{Ext}    &
 \rotatebox[origin=c]{270}{Loc}    &

 \rotatebox[origin=c]{270}{UB}                                 &
 \rotatebox[origin=c]{270}{\makecell{$\lPO\cup\lRF$ \\ acyc.}} & 
 \rotatebox[origin=c]{270}{OOTA}                               &                              


 \rotatebox[origin=c]{270}{NA}                      &
 \rotatebox[origin=c]{270}{RLX}                     &
 \rotatebox[origin=c]{270}{RA}                      &
 \rotatebox[origin=c]{270}{SC}                      &
 \rotatebox[origin=c]{270}{F-RA}                    &
 \rotatebox[origin=c]{270}{F-SC}                    &
 \rotatebox[origin=c]{270}{RMW}                     &
 \rotatebox[origin=c]{270}{Lock}                    &
 \rotatebox[origin=c]{270}{\makecell{Mix.Sz.}}      \\ 
 
 \Xhline{2\arrayrulewidth}
 
 \multirow{3}{*}{\rotatebox[origin=c]{270}{\makecell{SeqCst}}}   

 & SC             & & & & & & & & & & & & & & & & & & & & & & & & & & & & & \\ \cline{2-31}
 & SC-Haskell     & & & & & & & & & & & & & & & & & & & & & & & & & & & & & \\ \cline{2-31}
 & DRFx           & & & & & & & & & & & & & & & & & & & & & & & & & & & & & \\ \Xhline{2\arrayrulewidth}

 \multirow{2}{*}{\rotatebox[origin=c]{270}{\makecell{TSO\\PSO}}}   

 & BMM            & & & & & & & & & & & & & & & & & & & & & & & & & & & & & \\ \cline{2-31}

 & Rlx Op.Sem.    & & & & & & & & & & & & & & & & & & & & & & & & & & & & & \\ \Xhline{2\arrayrulewidth}

 \multirow{5}{*}{\rotatebox[origin=c]{270}{\makecell{$\lPO\lRF$\\acyc}}}   

 & RC11           & & & & & & & & & & & & & & & & & & & & & & & & & & & & & \\ \cline{2-31}

 & ORC11          & & & & & & & & & & & & & & & & & & & & & & & & & & & & & \\ \cline{2-31}

 & OCaml MM       & & & & & & & & & & & & & & & & & & & & & & & & & & & & & \\ \cline{2-31}

 & JAM            & & & & & & & & & & & & & & & & & & & & & & & & & & & & & \\ \cline{2-31}

 & Rlx Compos.    & & & & & & & & & & & & & & & & & & & & & & & & & & & & & \\ \Xhline{2\arrayrulewidth}

 \multirow{2}{*}{\rotatebox[origin=c]{270}{\makecell{$\lPPO\lRF$\\acyc}}}   

 & LKMM           & & & & & & & & & & & & & & & & & & & & & & & & & & & & & \\ \cline{2-31}

 & OHMM           & & & & & & & & & & & & & & & & & & & & & & & & & & & & & \\ \Xhline{2\arrayrulewidth}

 \multirow{7}{*}{\rotatebox[origin=c]{270}{\makecell{no-OOTA}}}   

 & JMM            & & & & & & & & & & & & & & & & & & & & & & & & & & & & & \\ \cline{2-31}

 & Promising      & & & & & & & & & & & & & & & & & & & & & & & & & & & & & \\ \cline{2-31}

 & Weakestmo      & & & & & & & & & & & & & & & & & & & & & & & & & & & & & \\ \cline{2-31}

 & MRD            & & & & & & & & & & & & & & & & & & & & & & & & & & & & & \\ \cline{2-31}

 & P-P/S          & & & & & & & & & & & & & & & & & & & & & & & & & & & & & \\ \cline{2-31}

 & J/R            & & & & & & & & & & & & & & & & & & & & & & & & & & & & & \\ \cline{2-31}

 & Generative     & & & & & & & & & & & & & & & & & & & & & & & & & & & & & \\ \Xhline{2\arrayrulewidth}

 \multirow{5}{*}{\rotatebox[origin=c]{270}{\makecell{OOTA}}}   

 & C11            & & & & & & & & & & & & & & & & & & & & & & & & & & & & & \\ \cline{2-31}

 & JS MM          & & & & & & & & & & & & & & & & & & & & & & & & & & & & & \\ \cline{2-31}

 & RMC            & & & & & & & & & & & & & & & & & & & & & & & & & & & & & \\ \cline{2-31}

 & RAO            & & & & & & & & & & & & & & & & & & & & & & & & & & & & & \\ \cline{2-31}

 & Spec.Comp.     & & & & & & & & & & & & & & & & & & & & & & & & & & & & & \\ \Xhline{2\arrayrulewidth}


\end{tabular}
\caption{
  % \textit{T.P.} --- trace preserving.
  % \textit{R.I.} --- reordering of independent instructions.
  % \textit{R.E.} --- redundunt load/store elimination.
  % \textit{I.L.E.} --- irrelevant load elimination.
  % \textit{S.L.I.} --- speculative load introduction.
  % \textit{S.} --- strengthening.
  % \textit{R.M.} --- roach motel reordering.
  % \textit{R.P.} --- register promotion.
  % \textit{V.R.} --- value range analysis based optimizations.
  % \textit{T.I.} --- thread inlining (sequentialization).
  % \textit{Int.} --- internal.
  % \textit{Ext.} --- external.
  % \textit{Loc.} --- local.
  % \textit{UB} --- undefined behavior.
  % \textit{OOTA} --- out-of-thin air values.
  % \textit{Mix.Sz.} --- mixed-size accesses.
}
\label{table:summary}
\end{table*}

\end{landscape}

\twocolumn
