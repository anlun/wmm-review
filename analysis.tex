\section{Analysis}

In this section we present the detailed comparison 
of the considered programming language memory models. 
We summarize our findings in~\cref{table:summary}.
Each row of the table corresponds to a memory model, denoted by its abbreviation. 
Columns of the table correspond to the properties of the models discussed in~\cref{sec:background}.
We split the properties into several subgroups. 

The first group is devoted to optimality of compilation mappings
to target hardware architectures. In order to be concise, 
we chose the binary classification of optimality, 
that is, we classify the compilation scheme as either optimal or not,
in the following sense.
We chose the weakest possible access mode supported by the model
and consider the compilation scheme for the memory accesses annotated by this mode. 
For the memory models that treat racy non-atomic accesses
as undefined behavior, we consider the compilation mapping
for most relaxed access types the model provides.
This is because the catch-fire semantics for racy non-atomics 
trivially permits the most optimal compilation mappings (see~\cref{sec:bgrnd-ub}).
We say that compilation scheme is \emph{optimal} if the 
accesses annotated by the most relaxed mode 
can be compiled just as plain load and store instructions 
of the given hardware architecture (\ie without memory fences or extra dependencies). 

The second group is dedicated to soundness of various program transformations. 
The classification is also binary: a transformation is either sound or unsound 
in the given memory model (in a sense stated in~\cref{sec:bgrnd-opt-sound}).
Again, to be concise, we do not consider all the combinations 
of program tranformations and memory access modes. 
Instead, we consider the weakest possible accesses which have fully defined semantics. 
We further split the transformations into global and local as in~\cref{sec:bgrnd-opt-sound}.

The third group corresponds to reasoning principles guaranteed by the model. 
It includes the following properties. What kind of DRF guarantee the model provides.
(we distinguish the internal, external and local DRF theorems, see~\cref{sec:bgrnd-drf}).
Whether the model has undefined behaviors (see~\cref{sec:bgrnd-ub}).
Whether the model permits out-of-thin-air values (see~\cref{sec:bgrnd-oota}).

Finally, the last group enumerates the list of memory access modes 
and fences supported by the model, as well as whether the model 
supports read-modify-write operations, locks, and mixed-size accesses.

Driven by our analysis of the models' properties, we partition all models into five classes. 
\app{I think this is the place to name the classes and briefly introduce them.}
The models from the same class have similar compilation mappings, 
set of sound program transformations, and provided reasoning guarantees.
Our classes are ordered by the weakness of the memory models they consist of.  
The strongest models are located at the top rows of the table, 
while the weakest are at the bottom. 

We next discuss each class in more details
(note that the order is different from the one in the~\cref{table:summary}). 
\app{I think we should briefly explain why the order is different.}
We also give some insight on the relationship
between compilation scheme optimality, 
soundness of transformations and reasoning guarantees.
In particular, we explain why the support of some reasoning guarantees 
disables some program transformations and requires more heavyweight 
compilation mappings to hardware.

\subsection{Strong SC-like Models}

Sequential Consistency (\SC) is one of the most intuitive models of concurrency.
Under this model, one can represent the state of the system as 
a simple mapping from memory locations to their values. 
Then each outcome can be obtained by a sequential interleaved execution 
of thread's instructions. Each execution induces a total order 
on individual memory access operations consistent with per-thread program order
and, moreover, each read is guaranteed to read-from the most recent write. 

\SC renders many common transformations unsound, 
including all kind of instruction reorderings and 
common subexpression elimination~\cite{Marino-al:PLDI11, Sevcik-Aspinall:ECOOP08}.
The fact that instruction reorderings are forbidden 
makes the model expensive to implement on modern hardware
since even relatively strong hardware model of x86
permits store/load reorderings.
Therefore, in order to preserve sequential consistency during compilation,
the compiler need to emit heavyweight memory fences between store and load instructions,
which makes compilation mappings far from being optimal.  

In terms of reasoning guarantees, however, SC is quite a pleasant model. 
It gives the DRF-SC and coherence properties for free.
The SC semantics assigns to programs only sequantially consistent
outcomes by definition, thus it satisfies DRF-SC without any preconditions.
The coherence property is equivalent to sequential consistency per location.
The fact that in SC model any execution is sequentially consistent implies trivially
that accesses to each location are also sequentially consistent.
The model also preserves program order.
\app{I don't get the last sub-sentence.}
\eupp{Use some nice non-techical terminology for $\lPO\cup\lRF$ acyclicity}

The conceptual simplicity of SC have inspired many researchers 
to adopt it and to try to mitigate the induced performance penalty.
The recurring idea was to somehow separate thread-local and shared mutable memory.
Accesses to the thread-local memory can be compiled 
without memory fences and are subject to a wider range 
of local transformations.
To safely distinguish between the two types of memory
researches proposed to utilize type-systems~\cite{Vollmer-al:PPoPP17},
static~\cite{Singh-al:ISCA12} or dynamic~\cite{Liu-al:PLDI19} analysis,
hardware support~\cite{Singh-al:ISCA12, Marino-al:PLDI10}
or some combination of the above. 

Despite these efforts \SC still induces considerable slowdown,
especially on weak hardware like ARMv8~\cite{Liu-al:PLDI19} 
(see \cref{sec:catalog:sc} for details).
Moreover, while these optimization typically reduce 
the penalties on thread-local accesses (a common case), 
they are likely to have lesser impact on specific 
applications which heavily utilize concurrency,
like, for example, lock-free data structures.
Finally, modern compilers usually require 
a significant amount of engeneering work and rewrite
in order to preserve \SC~\cite{Marino-al:PLDI11, Liu-al:PLDI19}.

\app{As a reader, I'd expect to see the slowdown numbers in the table in some
unified manner.}

\subsection{Strong TSO-like Models}

The next class of PL memory models we consider 
was inspired by \emph{total store order}~\cite{Sewell-al:CACM10} (\TSO) 
and \emph{partial store order}~\cite{Sparc:94} (\PSO) hardware models. 
In these models threads are equipped with \emph{store buffers}.
All store operations go to these buffers before they 
propogate into the main memory.
The name of these models stems from their axiomatic formulation. 
They require an existence of total (in case of \TSO) 
or partial (in case of \PSO) order on write events, 
consistent with program order and such that 
each read either reads locally or from 
the most recent write~\cite{Sewell-al:CACM10, Lahav-al:POPL16}. 

Models of this class can be compiled down to x86 hardware without any 
performance penalty, since x86 implements TSO model itself.
However, on weaker hardware (e.g. POWER) 
the compiler need to emit as many fences 
as to enforce \SC~\cite{Lustig-al:AISCA15}. 
This class also permits more program transformations.
In essence, store buffers enable store/load reordering (in case of~\TSO),
and store/load \& store/store reordegins (in case of~\PSO).
The \TSO and \PSO models are weaker than \SC, 
but they are still relatively strong.
The external DRF-SC and coherence properties hold
and program order is preserved.

Therefore these models do not have any significant 
benefits in terms of reasoning guarantees compared to \SC,
but induce a similar performance penalty on weak architectures. 
Hence the choice of \TSO and \PSO as the programming language level
memory model is reasonable only if the language targets x86 hardware solely. 

\subsection{OOTA Models}

We next move on to the other end of the memory models' spectrum. 
We consider the class uniting the weakest models of our analysis.
These models enable efficient compilation mappings and 
almost all reasonable program transformations, but at the cost of 
introducing so-called thin-air values (hence the name of this class).

In order to introduce the concept of thin-air values, 
let us first consider the two programs below. 

\begin{minipage}{0.43\linewidth}
\begin{equation*}
\small
\inarrII{
  \readInst{}{r_1}{x}     \\
  \writeInst{}{y}{1}      \\
}{
  \readInst{}{r_2}{y}     \\
  \writeInst{}{x}{r_2}    \\
}
\tag{LB}\label{ex:lb}
\end{equation*}
\end{minipage}\hfill%
\begin{minipage}{0.09\linewidth}
\Large~\\ $\leadsto$
\end{minipage}\hfill%
\begin{minipage}{0.43\linewidth}
\begin{equation*}
\inarrII{
  \writeInst{}{y}{1}      \\
  \readInst{}{r_1}{x}     \\
}{
  \readInst{}{r_2}{y}     \\
  \writeInst{}{x}{r_2}    \\
}
\tag{LBtr}\label{ex:lbtr}
\end{equation*}
\end{minipage}

Program on the right (\ref{ex:lbtr}) can be obtained 
from the program on the left (\ref{ex:lb})
via the load/store reordering.
An outcome $[r_1=1, r_2=1]$ is valid for \ref{ex:lbtr}.
Therefore under a memory model where load/store is a sound transformation, 
this outcome should also be valid for \ref{ex:lb}.
In order to enable this kind of behaviors for programs, 
the memory models usually utilize some form of 
\emph{speculative execution}~\cite{Boudol-Petri:ESOP10, Crary-Sullivan:POPL15}.
That is, during the execution, the load $\readInst{}{r_1}{x}$
is buffered and store $\writeInst{}{y}{1}$ is executed out of order
(hence the name of the program LB --- \emph{load buffering}).

However, unrestricted speculations can lead to disruptive results. 
A store executed out of order can turn into 
a self-satisfying prophecy~\cite{Boehm-Demsky:MSPC14}.
Consider the following variation of the load buffering program. 

\begin{equation*}
\small
\inarrII{
  \readInst{}{r_1}{x}   \\
  \writeInst{}{y}{r_1}  \\
}{
  \readInst{}{r_2}{y}   \\
  \writeInst{}{x}{r_2}  \\
}
\tag{LB+data}\label{ex:lb+data}
\end{equation*}

Here, a hypothetical abstract machine can speculate 
to perform a store of value \texttt{1} into variable \texttt{y}
from the left thread, then read this value in the right thread, 
write it to variable \texttt{x} and then read it back in the
left thread closing the paradoxical causality cycle.
The value \texttt{1} in the example above appears \emph{out of thin-air}
and then justifies itself leading to a confusing outcome.
 
Examples of this kind demonstrate the main common 
disadvantage of OOTA models.
They lack the fundamental reasoning 
principles~\cite{Boehm-Demsky:MSPC14, Batty-al:ESOP15}:
type safety and security gurantess can be violated, 
and compositional reasoning is impossible.
They also do not satisfy external DRF-SC property.
To see this consider yet another program.

\begin{equation*}
\inarrII{
  \readInst{}{r_1}{x}      \\
  \kw{if} {(r_1)} ~\{      \\
  \quad\writeInst{}{y}{1}  \\
  \}
}{
  \readInst{}{r_2}{x}      \\
  \kw{if} {(r_2)} ~\{      \\
  \quad\writeInst{}{x}{1}  \\
  \}
}
\tag{LB+ctrl}\label{ex:lb+ctrl}
\end{equation*}

For the memory model admitting thin-air values 
(as \eg \CMM~\cite{Batty-al:POPL11}), 
the outcome $[r_1=1, r_2=1]$ is valid
(one can see that the program above is analogous 
to the \ref{ex:lb+data}, except it has 
control dependencies between instructions 
in place of data dependencies).
Not only this outcome is completely unintuitive,
but it also contradicts to the external DRF-SC guarantee.
Indeed, under \SC model the program above has 
a single valid execution with the outcome $[r_1=0, r_2=0]$ 
and no data-races, thus under DRF-SC compliant model 
it should also has this sole outcome.  

Counter-intuitive behavior of OOTA models, together with the fact that they break 
many important reasoning principles,
has lead over the time to the consensus in the research community that these models 
are not suited well for the role of 
programming languages memory models~\cite{Boehm-Demsky:MSPC14, Batty-al:ESOP15}.
A lot of effort has been put to forbid problematic 
thin-air outcomes, while still keep compilation scheme as efficient as possible
and enable as many transformations as possible.

\subsection{$\lPO\cup\lRF$ Acyclic Models}
\label{sec:porf-acyc}

The most straightforward way to forbid thin-air values 
was proposed by Boehm and Demsky~\cite{Boehm-Demsky:MSPC14}
The idea is to simply prohibit any kind of speculative execution, 
which is equivalent to forbidding load/store reorderings altogether. 
This fix not only restores external DRF-SC~\cite{Lahav-al:PLDI17}
and other reasoning guarantees, but also leads to 
a much simpler model. The abstract machine implementing 
the memory model does not need to resort to speculative execution 
and can perform threads' instructions in order. 
The memory storage can be implemented as a 
monotonically growing history of messages, 
witch each thread having its own view on 
the frontier of this history~\cite{Dolan-al:PLDI18, Doherty-al:PPoPP19}.
In terms of axiomatic semantics it is enough to just
forbid cycles consisting of program order and reads-from relations. 

Lahav~\etal~\cite{Lahav-al:PLDI17} formalized this approach
to thin-air problem and studied it extensively. 
The authors have shown that many 
program transformations are still sound in this setting, 
with the obvious exception of load/store reordering itself
(see~\cref{table:summary} for details).
Also, the compilation mappings to \xTSO remain efficient, 
since the architecture already guarantee to preserve the order 
between loads and subsequent stores. 
However, weaker architectures (\ARM, \POWER) do not guarantee that, 
and thus additional measures are required.
Lahav~\etal~\cite{Lahav-al:PLDI17} proposed to compile relaxed load 
as plain load followed by a spurious conditional branch,
which introduces fake control dependency between 
the load and subsequent stores. 
ARM and POWER hardware preserves dependencies, 
and thus it has to also retain the load/store ordering. 

Ou and Demsky~\cite{Ou-Demsky:OOPSLA18} have studied 
the performance penalty required to preserve
load/store ordering between relaxed atomics 
and reported a negligible overhead 
(see \ref{sec:catalog:porf} for details).
However, the overhead is likely to be more significant
if one want to apply it to non-atomic accesses as well. 

In conclusion, the memory models forbidding load/store reorderings
provide a simple solution to thin-air problem, 
give a convinient formal model,  
restore external DRF-SC guarantee, preserve soundness of 
most program transformations, induce no overhead when 
compiled down to \xTSO, and only a moderate overhead 
when compiled to \ARM or \POWER.

\subsection{$\lPPO\cup\lRF$ Acyclic Models}
\label{sec:pporf-acyc}

An alternative conceptually simple solution 
to thin-air values problem is to preserve 
\emph{syntactic dependencies}~\cite{Boehm-Demsky:MSPC14, Alglave-al:ASPLOS18}.
Under this approach reordering of independent load/store pairs is not forbidden.
However, the reordering is forbidden if the store depends on the value 
read by the load either because this value 
was used to compute the value written by the store (\emph{data dependency}), 
or the memory address used in the store (\emph{address dependency}),
or else the control-flow path lead to the store was dependent
on this value (\emph{control dependency}).
For example, giving the program \ref{ex:lb+data} 
the store $\writeInst{}{y}{r_1}$ depends 
on the load $\readInst{}{x}{r_1}$ since 
it writes the value read by the load.

Note that these kind of dependencies are computed following the 
syntax of the program (hence the name) as opposed 
to \emph{semantic dependencies}.
For example, giving the modified version of the \ref{ex:lb+data} program below, 
the store to \texttt{y} is still considered to be dependent on the previous load. 

\begin{equation*}
\inarrII{
  \readInst{}{r_1}{x}           \\
  \writeInst{}{y}{1 + 0 * r_1}  \\
}{
  \readInst{}{r_2}{x}      \\
  \writeInst{}{x}{r_2}     \\
}
\tag{LB+fakedata}\label{ex:lb+fakedata}
\end{equation*}

Here the syntactic dependency can be eliminated 
by the \emph{constant folding} transformation --- 
the expression $1 + 0 * r_1$ can be reduced to just~$1$.
Under the syntactic dependency preserving memory model 
the compiler, however, is prohibited to perform this optimization. 
Indeed, once the dependency is removed, nothing prevents 
to reorder the store before the preceding load. 
Even if the compiler itself does not perform this reordering,
after the compilation the hardware can do this during the execution.   

This subtlety reveals the main drawback of 
syntactic dependency tracking models --- 
various trace preserving transformations
(\eg constant folding) are unsound in these models. 
Constant folding is one of the basic kind of optimizations that 
any compiler might want to apply, 
and the fact that it is unsound  
makes the adoption of this class of models problematic.
Ou and Demsky~\cite{Ou-Demsky:OOPSLA18} adjusted 
the compiler optimization passes to preserve
dependencies between non-atomic accesses 
and reported a moderate slowdown compared 
to the unmodified version of the compiler  
(see \ref{sec:catalog:pporf} for details).

Note that harware models apply a similar approach 
and usually have a notion of dependencies between 
the memory operations~\cite{Sarkar-al:PLDI11, Alglave-al:TOPLAS14, Pulte-al:POPL18}.
Yet in this setting the unsoundness of 
trace preserving transformation is not a problem,
since the hardware does not perform such complex optimizations.


\subsection{no-OOTA Models}
\label{sec:prm-cert}

The last approach to tackle thin-air problem is to   
construct a notion of \emph{semantic dependencies}, 
which would precisely characterize what load/store 
pairs are ``independent'' and rule out 
``fake'' dependencies like the one in \ref{ex:lb+fakedata}.
The practical payoff of this approach is that it 
does not require significant modifications to existing compilers or hardware, 
\app{Here should be some reference to optimality of compilation schemes used in the compilers}
and thus should not impose performance penalties.  
The ultimate goal is to enable optimal compilation mappings, 
preserve most of the existing compiler optimizations, 
and at the same time maintain the important 
reasoning guarantees like external DRF. 

It turns out that this task is quite challenging 
and to this date there is no strong consensus on how to achieve it.
In order to give a satisfactory definition of semantic dependencies 
the researchers had to resort to conceptually complex memory models%
~\cite{Jagadeesan-al:ESOP10, Kang-al:POPL17, Jeffrey-Riely:LICS16, 
PichonPharabod-Sewell:POPL16, Chakraborty-Vafeiadis:POPL19, Paviotti-al:ESOP20}.
The main challenge in this line of work was to formally prove 
that these complex models indeed satisfy all the desired properties. 

Currently the most complete approach of this class 
is the \Promising semantics~\cite{Kang-al:POPL17, Lee-al:PLDI20}. 
This model was proven to enable optimal compilation schemes~\cite{Podkopaev-al:POPL19}, 
and permit most local and global program transformations
(with the notable exception of thread inlining), 
while still preserving the external DRF-SC guarantee.

\subsection{Secondary Classes}
\label{sec:other-classes}

We also identify a secondary classes of memory models. 
Those correspond to a particular property of the memory model, 
\eg coherence or a catch-fire semantics which treats racy programs 
as errorneous. We demonstrate how presence or absence of these properties 
affects the soundness of program transformations and compilation mappings.

\subsubsection{Coherent Models}
\label{sec:analysis:coh}

Coherence property (\ie \SC-per-location, \cref{sec:background:coh})
has a subtle effect on the common subexpression elimination optimization (\CSE),
which was was first observed in the context of an early version of \Java 
memory model~\cite{Pugh:JAVA99}.
To see the problem, consider the program below
(on the left) and the transformed version 
of this program after application of CSE (on the right).
Note that the optimization has replaced 
the second access to variable \texttt{x}
by a read from register. 

\begin{minipage}{0.45\linewidth}
\begin{equation*}
\small
\inarrII{
  \readInst{}{r_1}{x}      \\
  \readInst{}{r_2}{y}      \\
  \readInst{}{r_3}{x}      \\
}{
  \writeInst{}{y}{1}       \\
}
\label{ex:coh-rr}
\end{equation*}
\end{minipage}\hfill%
\begin{minipage}{0.05\linewidth}
\Large~\\ $\leadsto$
\end{minipage}\hfill%
\begin{minipage}{0.45\linewidth}
\begin{equation*}
\small
\inarrII{
  \readInst{}{r_1}{x}      \\
  \readInst{}{r_2}{y}      \\
  \assignInst{r_3}{r_1}    \\
}{
  \writeInst{}{y}{1}       \\
}
\label{ex:coh-rr}
\end{equation*}
\end{minipage}

Now assume that \texttt{x} and \texttt{y} point to the same memory location.
Under this assumption the outcome $[r_1=0, r_2=1, r_3=0]$
is forbidden for the memory model respecting coherence.
Indeed, the coherence guarantees sequential consistency per location, 
which means that for the programs consisting of accesses 
to the single memory location 
(as the one above in the presence of aliasing) 
only the sequentially consistent outcomes are allowed.
The outcome $[r_1=0, r_2=1, r_3=0]$ cannot be obtained 
by the interleaving of instructions, and thus 
it should be forbidden.  
However, this outcome is allowed for 
the optimized version of the program. 

Note that the compiler still can apply \CSE to the program above, 
but only if it is able to prove that variables \texttt{x} and \texttt{y} 
point to disjoint memory locations. 
To derive this knowledge the compiler can utilize alias analysis~\cite{?},
which implementation is known to be challenging~\cite{?}.

Therefore, the coherence property in general is not compatible 
with common subexpression elimination, or at least 
it requires some adjustments of this optimization.

As for compilation schemes, coherence does not require 
any changes here and thus does not impose any performance penatly.
It is because the hardware already guarantee coherence%
~\cite{Alglave-al:TOPLAS14, Sarkar-al:PLDI11, Sewell-al:CACM10, Lahav-al:PLDI17}. 

\newpage
\onecolumn

\begin{landscape}

\begin{table*}
\begin{tabular}{|c|l|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|}
 \hline

                                                      &
 \multirow{3}{*}{Model}                               & 
 \multicolumn{ 4}{c|}{\multirow{2}{*}{Compilation}}   &
 \multicolumn{10}{c|}{Transformations}                &
 \multicolumn{ 6}{c|}{Reasoning}                      &
 \multicolumn{ 9}{c|}{\multirow{2}{*}{Features}}      \\ 

 \cline{7-22}

                             &
                             &
 \multicolumn{4}{c|}{}       &
 \multicolumn{7}{c|}{Local}  &
 \multicolumn{3}{c|}{Global} &

 \multicolumn{3}{c|}{DRF}    &
 \multicolumn{3}{c|}{}       &
 \multicolumn{9}{c|}{}       \\ 
 
 \hline
                                     &
                                     &
 \rotatebox[origin=c]{270}{x86}      & 
 \rotatebox[origin=c]{270}{Power}    & 
 \rotatebox[origin=c]{270}{ARMv7}    & 
 \rotatebox[origin=c]{270}{ARMv8}    & 
 
 \rotatebox[origin=c]{270}{TP}     &
 \rotatebox[origin=c]{270}{RI}     &
 \rotatebox[origin=c]{270}{RE}     &
 \rotatebox[origin=c]{270}{ILE}    &
 \rotatebox[origin=c]{270}{SLI}    &
 \rotatebox[origin=c]{270}{S}      &
 \rotatebox[origin=c]{270}{RM}     &
 \rotatebox[origin=c]{270}{RP}     &
 \rotatebox[origin=c]{270}{VR}     &
 \rotatebox[origin=c]{270}{TI}     &
 
 \rotatebox[origin=c]{270}{Int}    &
 \rotatebox[origin=c]{270}{Ext}    &
 \rotatebox[origin=c]{270}{Loc}    &

 \rotatebox[origin=c]{270}{UB}                                 &
 \rotatebox[origin=c]{270}{\makecell{$\lPO\cup\lRF$ \\ acyc.}} & 
 \rotatebox[origin=c]{270}{OOTA}                               &                              


 \rotatebox[origin=c]{270}{NA}                      &
 \rotatebox[origin=c]{270}{RLX}                     &
 \rotatebox[origin=c]{270}{RA}                      &
 \rotatebox[origin=c]{270}{SC}                      &
 \rotatebox[origin=c]{270}{F-RA}                    &
 \rotatebox[origin=c]{270}{F-SC}                    &
 \rotatebox[origin=c]{270}{RMW}                     &
 \rotatebox[origin=c]{270}{Lock}                    &
 \rotatebox[origin=c]{270}{\makecell{Mix.Sz.}}      \\ 
 
 \Xhline{2\arrayrulewidth}
 
 \multirow{3}{*}{\rotatebox[origin=c]{270}{\makecell{SeqCst}}}   

 & SC             & & & & & & & & & & & & & & & & & & & & & & & & & & & & & \\ \cline{2-31}
 & SC-Haskell     & & & & & & & & & & & & & & & & & & & & & & & & & & & & & \\ \cline{2-31}
 & DRFx           & & & & & & & & & & & & & & & & & & & & & & & & & & & & & \\ \Xhline{2\arrayrulewidth}

 \multirow{2}{*}{\rotatebox[origin=c]{270}{\makecell{TSO\\PSO}}}   

 & BMM            & & & & & & & & & & & & & & & & & & & & & & & & & & & & & \\ \cline{2-31}

 & Rlx Op.Sem.    & & & & & & & & & & & & & & & & & & & & & & & & & & & & & \\ \Xhline{2\arrayrulewidth}

 \multirow{5}{*}{\rotatebox[origin=c]{270}{\makecell{$\lPO\lRF$\\acyc}}}   

 & RC11           & & & & & & & & & & & & & & & & & & & & & & & & & & & & & \\ \cline{2-31}

 & ORC11          & & & & & & & & & & & & & & & & & & & & & & & & & & & & & \\ \cline{2-31}

 & OCaml MM       & & & & & & & & & & & & & & & & & & & & & & & & & & & & & \\ \cline{2-31}

 & JAM            & & & & & & & & & & & & & & & & & & & & & & & & & & & & & \\ \cline{2-31}

 & Rlx Compos.    & & & & & & & & & & & & & & & & & & & & & & & & & & & & & \\ \Xhline{2\arrayrulewidth}

 \multirow{2}{*}{\rotatebox[origin=c]{270}{\makecell{$\lPPO\lRF$\\acyc}}}   

 & LKMM           & & & & & & & & & & & & & & & & & & & & & & & & & & & & & \\ \cline{2-31}

 & OHMM           & & & & & & & & & & & & & & & & & & & & & & & & & & & & & \\ \Xhline{2\arrayrulewidth}

 \multirow{7}{*}{\rotatebox[origin=c]{270}{\makecell{no-OOTA}}}   

 & JMM            & & & & & & & & & & & & & & & & & & & & & & & & & & & & & \\ \cline{2-31}

 & Promising      & & & & & & & & & & & & & & & & & & & & & & & & & & & & & \\ \cline{2-31}

 & Weakestmo      & & & & & & & & & & & & & & & & & & & & & & & & & & & & & \\ \cline{2-31}

 & MRD            & & & & & & & & & & & & & & & & & & & & & & & & & & & & & \\ \cline{2-31}

 & P-P/S          & & & & & & & & & & & & & & & & & & & & & & & & & & & & & \\ \cline{2-31}

 & J/R            & & & & & & & & & & & & & & & & & & & & & & & & & & & & & \\ \cline{2-31}

 & Generative     & & & & & & & & & & & & & & & & & & & & & & & & & & & & & \\ \Xhline{2\arrayrulewidth}

 \multirow{5}{*}{\rotatebox[origin=c]{270}{\makecell{OOTA}}}   

 & C11            & & & & & & & & & & & & & & & & & & & & & & & & & & & & & \\ \cline{2-31}

 & JS MM          & & & & & & & & & & & & & & & & & & & & & & & & & & & & & \\ \cline{2-31}

 & RMC            & & & & & & & & & & & & & & & & & & & & & & & & & & & & & \\ \cline{2-31}

 & RAO            & & & & & & & & & & & & & & & & & & & & & & & & & & & & & \\ \cline{2-31}

 & Spec.Comp.     & & & & & & & & & & & & & & & & & & & & & & & & & & & & & \\ \Xhline{2\arrayrulewidth}


\end{tabular}
\caption{
  % \textit{T.P.} --- trace preserving.
  % \textit{R.I.} --- reordering of independent instructions.
  % \textit{R.E.} --- redundunt load/store elimination.
  % \textit{I.L.E.} --- irrelevant load elimination.
  % \textit{S.L.I.} --- speculative load introduction.
  % \textit{S.} --- strengthening.
  % \textit{R.M.} --- roach motel reordering.
  % \textit{R.P.} --- register promotion.
  % \textit{V.R.} --- value range analysis based optimizations.
  % \textit{T.I.} --- thread inlining (sequentialization).
  % \textit{Int.} --- internal.
  % \textit{Ext.} --- external.
  % \textit{Loc.} --- local.
  % \textit{UB} --- undefined behavior.
  % \textit{OOTA} --- out-of-thin air values.
  % \textit{Mix.Sz.} --- mixed-size accesses.
}
\label{table:summary}
\end{table*}

\end{landscape}

\twocolumn
